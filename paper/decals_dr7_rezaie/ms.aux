\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\citation{efstathiou1988analysis,fisher1993power,smoot1992structure,mather1994measurement,riess1998observational,perlmutter1999measurements,ata2017clustering,BOSSfinal,jones2018measuring,akrami2018planck,Elvin18}
\citation{peebles1973statistical,kaiser1987clustering,mukhanov1992theory,hamilton1998linear,eisenstein1998cosmic,seo2003probing,eisenstein2005dark,sanchez2008best,dalal2008imprints}
\citation{york2000sloan,colless20012df,drinkwater2010wigglez}
\citation{aghamousa2016desi}
\citation{ivezic2008lsst,LSSTObservingStrategyWhitePaper}
\@LN@col{1}
\newlabel{firstpage}{{}{1}{}{Doc-Start}{}}
\@LN{0}{0}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@LN{1}{0}
\@LN{2}{0}
\@LN{3}{0}
\@LN{4}{0}
\@LN{5}{0}
\@LN{6}{0}
\@LN{7}{0}
\@LN{8}{0}
\@LN{9}{0}
\@LN{10}{0}
\@LN{11}{0}
\@LN{12}{0}
\@LN{13}{0}
\@LN{14}{0}
\@LN{15}{0}
\@LN@col{2}
\@LN{16}{0}
\@LN{17}{0}
\@LN{18}{0}
\@LN{19}{0}
\@LN{20}{0}
\@LN{21}{0}
\@LN{22}{0}
\@LN{23}{0}
\@LN{24}{0}
\@LN{25}{0}
\@LN{26}{0}
\@LN{27}{0}
\@LN{28}{0}
\@LN{29}{0}
\@LN{30}{0}
\@LN{31}{0}
\@LN{32}{0}
\@LN{33}{0}
\@LN{34}{0}
\@LN{35}{0}
\citation{myers2007clustering,thomas2011angular,thomas2011excess,ross2011ameliorating,ashley2012MNRAS,2012ApJ...761...14H,huterer2013calibration,pullen2013systematic}
\citation{rybicki1992interpolation,tegmark1997measure,tegmark1998measuring,slosar2004exact,ho2008correlation,pullen2013systematic,leistedt2013estimating,leistedt2014exploiting}
\citation{leistedt2013estimating}
\citation{leistedt2014exploiting}
\citation{elsner2015unbiased}
\citation{kalus2016unbiased}
\citation{kalus2018map}
\citation{ross2011ameliorating,ashley2012MNRAS,Ross17,2012ApJ...761...14H,delubac2016sdss,prakash2016sdss,Raichoor2017MNRAS.471.3955R,laurent2017clustering,Elvin18,2018ApJ...863..110B}
\citation{2012ApJ...761...14H}
\citation{rossfNL}
\citation{ahn2012ninth}
\citation{Elvin18}
\citation{berge2013ultra,Balrog}
\citation{devijver1982pattern,john1994irrelevant,koller1996toward,kohavi1997wrappers,ramaswamy2001multiclass,guyon2003introduction}
\citation{dey2018overview}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{dey2018overview}
\citation{wright2010wide}
\citation{lang2016tractor,aghamousa2016desi}
\citation{zou2017project}
\citation{dey2018overview}
\citation{dark2005dark}
\@LN@col{1}
\@LN{36}{1}
\@LN{37}{1}
\@LN{38}{1}
\@LN{39}{1}
\@LN{40}{1}
\@LN{41}{1}
\@LN{42}{1}
\@LN{43}{1}
\@LN{44}{1}
\@LN{45}{1}
\@LN{46}{1}
\@LN{47}{1}
\@LN{48}{1}
\@LN{49}{1}
\@LN{50}{1}
\@LN{51}{1}
\@LN{52}{1}
\@LN{53}{1}
\@LN{54}{1}
\@LN{55}{1}
\@LN{56}{1}
\@LN{57}{1}
\@LN{58}{1}
\@LN{59}{1}
\@LN{60}{1}
\@LN{61}{1}
\@LN{62}{1}
\@LN{63}{1}
\@LN{64}{1}
\@LN{65}{1}
\@LN{66}{1}
\@LN{67}{1}
\@LN{68}{1}
\@LN{69}{1}
\@LN{70}{1}
\@LN{71}{1}
\@LN{72}{1}
\@LN{73}{1}
\@LN{74}{1}
\@LN{75}{1}
\@LN{76}{1}
\@LN{77}{1}
\@LN{78}{1}
\@LN{79}{1}
\@LN{80}{1}
\@LN{81}{1}
\@LN{82}{1}
\@LN{83}{1}
\@LN{84}{1}
\@LN{85}{1}
\@LN{86}{1}
\@LN{87}{1}
\@LN{88}{1}
\@LN{89}{1}
\@LN{90}{1}
\@LN{91}{1}
\@LN{92}{1}
\@LN{93}{1}
\@LN{94}{1}
\@LN{95}{1}
\@LN{96}{1}
\@LN{97}{1}
\@LN@col{2}
\@LN{98}{1}
\@LN{99}{1}
\@LN{100}{1}
\@LN{101}{1}
\@LN{102}{1}
\@LN{103}{1}
\@LN{104}{1}
\@LN{105}{1}
\@LN{106}{1}
\@LN{107}{1}
\@LN{108}{1}
\@LN{109}{1}
\@LN{110}{1}
\@LN{111}{1}
\@LN{112}{1}
\@LN{113}{1}
\@LN{114}{1}
\@LN{115}{1}
\@LN{116}{1}
\@LN{117}{1}
\@LN{118}{1}
\@LN{119}{1}
\@LN{120}{1}
\@LN{121}{1}
\@LN{122}{1}
\@LN{123}{1}
\@LN{124}{1}
\@LN{125}{1}
\@LN{126}{1}
\@LN{127}{1}
\@LN{128}{1}
\@LN{129}{1}
\@LN{130}{1}
\@LN{131}{1}
\@LN{132}{1}
\@LN{133}{1}
\@LN{134}{1}
\@LN{135}{1}
\@LN{136}{1}
\@LN{137}{1}
\@LN{138}{1}
\@LN{139}{1}
\@LN{140}{1}
\@LN{141}{1}
\@LN{142}{1}
\@LN{143}{1}
\@LN{144}{1}
\@LN{145}{1}
\@LN{146}{1}
\@LN{147}{1}
\@LN{148}{1}
\@LN{149}{1}
\@LN{150}{1}
\@LN{151}{1}
\@LN{152}{1}
\@LN{153}{1}
\@LN{154}{1}
\@LN{155}{1}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{dey2018overview}
\citation{gorski2005healpix}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{gorski2005healpix}
\citation{LeistedtMap}
\citation{schlegel1998maps}
\citation{brown2018gaia}
\citation{bekhti2016hi4pi}
\citation{schlegel1998maps}
\citation{schlafly2011measuring}
\citation{brown2018gaia}
\citation{ashley2012MNRAS}
\citation{bekhti2016hi4pi}
\@LN@col{1}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The Northern Galactic Cap color-magnitude selection of the eBOSS Emission Line Galaxies \citep  {Raichoor2017MNRAS.471.3955R}. We enforce the same selection for the entire sky. Note that our selection is slightly different from \citet  {Raichoor2017MNRAS.471.3955R} in the clean photometry criteria as explained in the main text.}}{3}{table.1}}
\newlabel{tab:ts}{{1}{3}{The Northern Galactic Cap color-magnitude selection of the eBOSS Emission Line Galaxies \citep {Raichoor2017MNRAS.471.3955R}. We enforce the same selection for the entire sky. Note that our selection is slightly different from \citet {Raichoor2017MNRAS.471.3955R} in the clean photometry criteria as explained in the main text}{table.1}{}}
\@LN{156}{2}
\@writefile{toc}{\contentsline {section}{\numberline {2}Legacy Surveys DR7}{3}{section.2}}
\newlabel{sec:data}{{2}{3}{Legacy Surveys DR7}{section.2}{}}
\@LN{157}{2}
\@LN{158}{2}
\@LN{159}{2}
\@LN{160}{2}
\@LN{161}{2}
\@LN{162}{2}
\@LN{163}{2}
\@LN{164}{2}
\@LN{165}{2}
\@LN{166}{2}
\@LN{167}{2}
\@LN{168}{2}
\@LN{169}{2}
\@LN{170}{2}
\@LN{171}{2}
\@LN{172}{2}
\@LN{173}{2}
\@LN{174}{2}
\@LN{175}{2}
\@LN{176}{2}
\@LN{177}{2}
\@LN{178}{2}
\@LN{179}{2}
\@LN{180}{2}
\@LN{181}{2}
\@LN{182}{2}
\@LN{183}{2}
\@LN{184}{2}
\@LN{185}{2}
\@LN{186}{2}
\@LN{187}{2}
\@LN{188}{2}
\@LN{189}{2}
\@LN{190}{2}
\@LN{191}{2}
\@LN{192}{2}
\@LN{193}{2}
\@LN{194}{2}
\@LN{195}{2}
\@LN{196}{2}
\@LN{197}{2}
\@LN{198}{2}
\@LN@col{2}
\@LN{199}{2}
\@LN{200}{2}
\@LN{201}{2}
\@LN{202}{2}
\@LN{203}{2}
\@LN{204}{2}
\@LN{205}{2}
\@LN{206}{2}
\@LN{207}{2}
\@LN{208}{2}
\@LN{209}{2}
\@LN{210}{2}
\@LN{211}{2}
\@LN{212}{2}
\@LN{213}{2}
\@LN{214}{2}
\@LN{215}{2}
\@LN{216}{2}
\@LN{217}{2}
\@LN{218}{2}
\@LN{219}{2}
\@LN{220}{2}
\@LN{221}{2}
\@LN{222}{2}
\@LN{223}{2}
\@LN{224}{2}
\@LN{225}{2}
\@LN{226}{2}
\@LN{227}{2}
\@LN{228}{2}
\@LN{229}{2}
\@LN{230}{2}
\@LN{231}{2}
\@LN{232}{2}
\@LN{233}{2}
\@LN{234}{2}
\@LN{235}{2}
\@LN{236}{2}
\@LN{237}{2}
\@LN{238}{2}
\@LN{239}{2}
\@LN{240}{2}
\@LN{241}{2}
\@LN{242}{2}
\@LN{243}{2}
\@LN{244}{2}
\@LN{245}{2}
\@LN{246}{2}
\@LN{247}{2}
\@LN{248}{2}
\@LN{249}{2}
\@LN{250}{2}
\@LN{251}{2}
\@LN{252}{2}
\@LN{253}{2}
\@LN{254}{2}
\@LN@col{1}
\@LN{255}{3}
\@LN{256}{3}
\@LN{257}{3}
\@LN{258}{3}
\@LN{259}{3}
\@LN{260}{3}
\@LN{261}{3}
\@LN{262}{3}
\@LN{263}{3}
\@LN{264}{3}
\@LN{265}{3}
\@LN{266}{3}
\@LN{267}{3}
\@LN{268}{3}
\@LN{269}{3}
\@LN{270}{3}
\@LN{271}{3}
\@LN{272}{3}
\@LN{273}{3}
\@LN{274}{3}
\@LN{275}{3}
\@LN{276}{3}
\@LN{277}{3}
\@LN{278}{3}
\@LN{279}{3}
\@LN{280}{3}
\@LN{281}{3}
\@LN{282}{3}
\@LN{283}{3}
\@LN{284}{3}
\@LN{285}{3}
\@LN{286}{3}
\@LN{287}{3}
\@LN{288}{3}
\@LN{289}{3}
\@LN{290}{3}
\newlabel{eq:comp}{{1}{4}{Legacy Surveys DR7}{equation.1}{}}
\newlabel{eq:depth_cuts}{{2}{4}{Legacy Surveys DR7}{equation.2}{}}
\@LN{291}{3}
\@LN{292}{3}
\@LN{293}{3}
\@LN{294}{3}
\@LN{295}{3}
\@LN@col{2}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The statistics of the DR7 imaging attributes used in this paper. Due to the non-Gaussian nature of the attributes, we report the mean, 15.9-, and 84.1-th percentile points of the imaging attributes.}}{4}{table.2}}
\newlabel{tab:meanstats}{{2}{4}{The statistics of the DR7 imaging attributes used in this paper. Due to the non-Gaussian nature of the attributes, we report the mean, 15.9-, and 84.1-th percentile points of the imaging attributes}{table.2}{}}
\newlabel{eq:pcc}{{3}{4}{Legacy Surveys DR7}{equation.3}{}}
\@LN{296}{3}
\@LN{297}{3}
\@LN{298}{3}
\@LN{299}{3}
\@LN{300}{3}
\@LN{301}{3}
\@LN{302}{3}
\@LN{303}{3}
\@LN{304}{3}
\@LN{305}{3}
\@LN{306}{3}
\@LN{307}{3}
\@LN{308}{3}
\@LN{309}{3}
\@LN{310}{3}
\@LN{311}{3}
\@LN{312}{3}
\@LN{313}{3}
\@LN{314}{3}
\@LN{315}{3}
\@LN{316}{3}
\@LN{317}{3}
\@LN{318}{3}
\@LN{319}{3}
\@LN{320}{3}
\@LN{321}{3}
\@LN{322}{3}
\@LN{323}{3}
\@LN{324}{3}
\citation{myers2007clustering,ross2011ameliorating,2012ApJ...761...14H,prakash2016sdss,2016MNRAS.455.4301C}
\citation{breiman2001random,geurts2006extremely}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textit  {Top panel}: the pixelated density map of the eBOSS-like ELGs from DR7 after correcting for the completeness of each pixel (see eq., \ref  {eq:comp}) and masking based on the survey depth and completeness cuts, see eq., \ref  {eq:depth_cuts}. The solid red curve represents the Galactic plane. This figure is generated by the code described in \url  {https://nbviewer.jupyter.org/github/desihub/desiutil/blob/master/doc/nb/SkyMapExamples.ipynb}. \textit  {Bottom panel}: the color-coded Pearson correlation matrix between each pair of the DR7 imaging attributes.}}{5}{figure.1}}
\newlabel{fig:eboss_dr7}{{1}{5}{\textit {Top panel}: the pixelated density map of the eBOSS-like ELGs from DR7 after correcting for the completeness of each pixel (see eq., \ref {eq:comp}) and masking based on the survey depth and completeness cuts, see eq., \ref {eq:depth_cuts}. The solid red curve represents the Galactic plane. This figure is generated by the code described in \url {https://nbviewer.jupyter.org/github/desihub/desiutil/blob/master/doc/nb/SkyMapExamples.ipynb}. \textit {Bottom panel}: the color-coded Pearson correlation matrix between each pair of the DR7 imaging attributes}{figure.1}{}}
\@LN@col{1}
\@LN{325}{4}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{5}{section.3}}
\newlabel{sec:method}{{3}{5}{Methodology}{section.3}{}}
\@LN{326}{4}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observed galaxy density}{5}{subsection.3.1}}
\@LN{327}{4}
\@LN{328}{4}
\@LN{329}{4}
\@LN{330}{4}
\@LN{331}{4}
\@LN{332}{4}
\@LN{333}{4}
\@LN{334}{4}
\@LN{335}{4}
\@LN{336}{4}
\@LN{337}{4}
\@LN{338}{4}
\@LN{339}{4}
\@LN{340}{4}
\@LN@col{2}
\newlabel{eq:ngal_fs}{{4}{5}{Observed galaxy density}{equation.4}{}}
\@LN{341}{4}
\@LN{342}{4}
\@LN{343}{4}
\@LN{344}{4}
\@LN{345}{4}
\@LN{346}{4}
\@LN{347}{4}
\@LN{348}{4}
\@LN{349}{4}
\@LN{350}{4}
\@LN{351}{4}
\@LN{352}{4}
\@LN{353}{4}
\@LN{354}{4}
\@LN{355}{4}
\citation{cybenko1989approximation,hornik1989multilayer,funahashi1989approximate,tamura1997capabilities,huang2003learning,lin2017does,rolnick2017power}
\citation{nair2010rectified,glorot2011deep,krizhevsky2012imagenet,dahl2013improving,montufar2014number}
\citation{he2015delving}
\citation{kingma2014adam}
\citation{ruder2016overview}
\@LN@col{1}
\@LN{356}{5}
\@LN{357}{5}
\newlabel{eq:overden}{{5}{6}{Observed galaxy density}{equation.5}{}}
\newlabel{eq:nobsi}{{6}{6}{Observed galaxy density}{equation.6}{}}
\newlabel{eq:nbar}{{7}{6}{Observed galaxy density}{equation.7}{}}
\@LN{358}{5}
\@LN{359}{5}
\@LN{360}{5}
\@LN{361}{5}
\@LN{362}{5}
\@LN{363}{5}
\@LN{364}{5}
\@LN{365}{5}
\@LN{366}{5}
\@LN{367}{5}
\@LN{368}{5}
\@LN{369}{5}
\newlabel{eq:nnbar}{{8}{6}{Observed galaxy density}{equation.8}{}}
\newlabel{eq:tfs}{{9}{6}{Observed galaxy density}{equation.9}{}}
\@LN{370}{5}
\@LN{371}{5}
\@LN{372}{5}
\@LN{373}{5}
\newlabel{eq:wsys}{{10}{6}{Observed galaxy density}{equation.10}{}}
\@LN{374}{5}
\@LN{375}{5}
\@LN{376}{5}
\@LN{377}{5}
\@LN{378}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Mitigation with Neural Networks}{6}{subsection.3.2}}
\newlabel{subsec:MethodNN}{{3.2}{6}{Mitigation with Neural Networks}{subsection.3.2}{}}
\@LN{379}{5}
\@LN{380}{5}
\@LN{381}{5}
\@LN{382}{5}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A schematic diagram of a single neuron with the activation function $f$. The neuron takes a set of inputs \textbf  {x}=$(x_{1}, x_{2},...,x_{n})$, multiplies each of them by its associated weight \textbf  {w}=$(w_{1}, w_{2},...,w_{n})$, and sums the weighted values and a thresh-hold (or a constant offset) which is called \textit  {bias}, to form a pre-activation value, $z=\DOTSI \sumop \slimits@ _{i=1}^{n}x_{i}w_{i} + bias$, which is a linear process. The neuron then transforms the pre-activation $z$ to the output using the activation function $f(z)$, which is where the nonlinear process can enter. }}{6}{figure.2}}
\newlabel{fig:perceptron}{{2}{6}{A schematic diagram of a single neuron with the activation function $f$. The neuron takes a set of inputs \textbf {x}=$(x_{1}, x_{2},...,x_{n})$, multiplies each of them by its associated weight \textbf {w}=$(w_{1}, w_{2},...,w_{n})$, and sums the weighted values and a thresh-hold (or a constant offset) which is called \textit {bias}, to form a pre-activation value, $z=\sum _{i=1}^{n}x_{i}w_{i} + bias$, which is a linear process. The neuron then transforms the pre-activation $z$ to the output using the activation function $f(z)$, which is where the nonlinear process can enter}{figure.2}{}}
\@LN{383}{5}
\@LN{384}{5}
\@LN{385}{5}
\@LN{386}{5}
\@LN{387}{5}
\@LN{388}{5}
\@LN{389}{5}
\@LN{390}{5}
\@LN{391}{5}
\@LN{392}{5}
\@LN{393}{5}
\@LN{394}{5}
\@LN{395}{5}
\@LN{396}{5}
\@LN{397}{5}
\@LN{398}{5}
\@LN{399}{5}
\@LN{400}{5}
\@LN{401}{5}
\@LN{402}{5}
\@LN{403}{5}
\@LN{404}{5}
\@LN{405}{5}
\@LN{406}{5}
\@LN{407}{5}
\@LN{408}{5}
\@LN{409}{5}
\@LN{410}{5}
\@LN{411}{5}
\@LN{412}{5}
\@LN{413}{5}
\@LN{414}{5}
\@LN{415}{5}
\@LN{416}{5}
\@LN{417}{5}
\citation{hoerl1970ridge}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A schematic illustration of a fully connected feed forward neural network with the imaging attributes in the input layer, three hidden layers of six neurons in the middle, and a single neuron on the output layer, as an example. The blue-colored neurons have non-linear activation functions, while the red-colored neuron lacks any activation function. In reality, we employ the validation procedure to choose the best number of hidden layers while keeping the total number of hidden layer neurons fixed at 40 (i.e., approximately twice the number of imaging attributes in this study).}}{7}{figure.3}}
\newlabel{fig:ffnn}{{3}{7}{A schematic illustration of a fully connected feed forward neural network with the imaging attributes in the input layer, three hidden layers of six neurons in the middle, and a single neuron on the output layer, as an example. The blue-colored neurons have non-linear activation functions, while the red-colored neuron lacks any activation function. In reality, we employ the validation procedure to choose the best number of hidden layers while keeping the total number of hidden layer neurons fixed at 40 (i.e., approximately twice the number of imaging attributes in this study)}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A visualization of the five-fold permutations of data-split. The data is randomly split into five equal-size folds, and by permutation of the folds we construct five partitions of data. For each partition/permutation, three folds are assigned to the training set, one fold for the validation set, and the remaining one fold for the test set: therefore, 60\% training, 20\% validation, and 20\% test sets. Each column represents a partition. The test folds are shown in red, while the training and validation folds are shown in blue. The key points are : 1) The folds are not contiguous (in RA, DEC) as may be implied by this cartoon. 2) There is no overlap between the training, validation, and test folds within a partition. 3) One can reconstruct the entire data by merging the test folds from the five partitions.}}{7}{figure.4}}
\newlabel{fig:5fold}{{4}{7}{A visualization of the five-fold permutations of data-split. The data is randomly split into five equal-size folds, and by permutation of the folds we construct five partitions of data. For each partition/permutation, three folds are assigned to the training set, one fold for the validation set, and the remaining one fold for the test set: therefore, 60\% training, 20\% validation, and 20\% test sets. Each column represents a partition. The test folds are shown in red, while the training and validation folds are shown in blue. The key points are : 1) The folds are not contiguous (in RA, DEC) as may be implied by this cartoon. 2) There is no overlap between the training, validation, and test folds within a partition. 3) One can reconstruct the entire data by merging the test folds from the five partitions}{figure.4}{}}
\@LN@col{2}
\@LN{418}{6}
\@LN{419}{6}
\@LN{420}{6}
\@LN{421}{6}
\@LN{422}{6}
\@LN{423}{6}
\@LN{424}{6}
\@LN{425}{6}
\@LN{426}{6}
\@LN{427}{6}
\@LN{428}{6}
\@LN{429}{6}
\@LN{430}{6}
\@LN{431}{6}
\@LN{432}{6}
\@LN{433}{6}
\@LN{434}{6}
\@LN{435}{6}
\newlabel{eq:cost}{{11}{7}{Mitigation with Neural Networks}{equation.11}{}}
\@LN{436}{6}
\@LN{437}{6}
\@LN{438}{6}
\@LN{439}{6}
\@LN{440}{6}
\@LN{441}{6}
\@LN{442}{6}
\@LN{443}{6}
\@LN{444}{6}
\@LN{445}{6}
\@LN{446}{6}
\@LN{447}{6}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Backward feature elimination}{7}{subsubsection.3.2.1}}
\@LN{448}{6}
\@LN{449}{6}
\@LN{450}{6}
\@LN{451}{6}
\@LN{452}{6}
\@LN{453}{6}
\@LN{454}{6}
\@LN{455}{6}
\@LN{456}{6}
\@LN{457}{6}
\@LN{458}{6}
\@LN{459}{6}
\@LN{460}{6}
\@LN{461}{6}
\@LN{462}{6}
\@LN{463}{6}
\@LN{464}{6}
\@LN{465}{6}
\@LN{466}{6}
\@LN{467}{6}
\@LN{468}{6}
\@LN{469}{6}
\@LN{470}{6}
\@LN{471}{6}
\@LN{472}{6}
