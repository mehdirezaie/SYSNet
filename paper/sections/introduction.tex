%--- introduction
\section{Introduction}\label{sec:intro}
Our current understanding of the Universe is founded upon statistical analyses of cosmological observables, such as the large-scale structure of galaxies, the cosmic microwave background, and the Hubble diagram of distant type Ia supernovae \citep[e.g.,][]{efstathiou1988analysis, fisher1993power, smoot1992structure, mather1994measurement,riess1998observational, perlmutter1999measurements, ata2017clustering, BOSSfinal, jones2018measuring, akrami2018planck, Elvin18}. Among these probes, galaxy surveys aim at constructing clustering statistics of galaxies with which we can investigate the dynamic of the cosmic expansion due to dark energy, test Einstein's theory of gravity, and constrain the total mass of neutrinos and statistical properties of the primordial fluctuations, etc \citep{peebles1973statistical,kaiser1987clustering,mukhanov1992theory,hamilton1998linear,eisenstein1998cosmic,seo2003probing,eisenstein2005dark,sanchez2008best, dalal2008imprints}.\\

The field of cosmology has been substantially advanced by the recent torrents of spectroscopic and imaging datasets from galaxy surveys such as the Sloan Digital Sky Survey (SDSS), Two Degree Field Galaxy Redshift Survey, and WiggleZ Dark Energy Survey \citep{york2000sloan, colless20012df, drinkwater2010wigglez}. The SDSS has been gathering data through different phases SDSS-I (2000-2005), SDSS-II (2005-2008), SDSS-III (2008-2014), and SDSS-IV (2014-2019). In order to derive more robust constraints with higher statistical confidence along with advancements in the technology of spectrographs, software, and computing machines, future large galaxy surveys aim at not only a wider area but also fainter galaxies out to higher redshifts. As an upcoming ground-based survey, the Dark Energy Spectroscopic Instrument (DESI) experiment will gather spectra of thirty million galaxies over 14,000 deg$^{2}$ starting in late 2019; this is approximately a factor of ten increase in the number of galaxy spectra compared to those observed in SDSS I--IV. This massive amount of spectroscopic data will lead to groundbreaking measurements of cosmological parameters through statistical data analyses of the clustering measurements of the 3D distribution of galaxies and quasars \citep{aghamousa2016desi}.\\ 


The Large Synoptic Survey Telescope (LSST) is another ground-based survey currently being constructed. It will gather 20 Terabytes of imaging data every night and will cover 18,000 deg$^{2}$ of the sky for ten years with a sample size of $2\times 10^{9}$ galaxies. The LSST would provide enough imaging data to address many puzzling astrophysical problems from the nature of dark matter, the growth of structure to our own Milky Way. Given such a data volume, many anticipate that the LSST will revolutionize the way astronomers do research and data analysis \citep{ivezic2008lsst, LSSTObservingStrategyWhitePaper}. \\

The enormous increased data volume provided by DESI and the LSST will significantly improve statistical confidences but will require analyses that are more complex and sensitive to the unknown systematic effects. A particular area of concern is the systematic effects due to imaging attributes such as atmospheric conditions, foreground stellar density, and/or inaccurate calibrations of magnitudes. These systematic effects can affect the target galaxy selections and therefore induce the non-cosmological perturbations into the galaxy density field, leading to excess clustering amplitudes, especially on large scales \citep[see e.g.][]{myers2007clustering,thomas2011angular,thomas2011excess, ross2011ameliorating, ashley2012MNRAS, 2012ApJ...761...14H, huterer2013calibration, pullen2013systematic}.\\ 


Robust and precise measurements of cosmological parameters from the large-scale galaxy clustering are contingent upon thorough treatment of such systematic effects. Many techniques have been developed to mitigate the effects. One can generally classify these methods into the mode projection, regression,  and Monte Carlo simulation of fake objects.\\ 

The mode-projection based techniques attribute a large variance to the spatial modes that strongly correlate with the potential systematic maps such as imaging attributes, thereby effectively removing those modes from the estimation of power spectrum \citep[see e.g.][]{rybicki1992interpolation,tegmark1997measure,tegmark1998measuring,slosar2004exact,ho2008correlation, pullen2013systematic,leistedt2013estimating,leistedt2014exploiting}. In detail, the basic mode projection \citep{leistedt2013estimating} produces an unbiased power spectrum and is equivalent to a marginalization over a free amplitude for the contamination produced by a given map.
 The caveat is that the variance of the estimated clustering increases by projecting out more modes for more imaging attributes. The \textit{extended} mode projection technique \citep{leistedt2014exploiting} resolves this issue by selecting a subset of the imaging maps using a $\chi^{2}$ threshold to determine the significance of a potential map. The limitation of the mode-projection based methods is that they are only applicable to the two-dimensional clustering measurements, and they reintroduce a small bias \citep{elsner2015unbiased}. \citet{kalus2016unbiased} extended the idea to the 3D clustering statistics and developed a new step to unbias the measurements~\citep[for an application on SDSS-III BOSS data see e.g.,][]{kalus2018map}\\

The regression-based techniques model the dependency of the galaxy density on the potential systematic fluctuations and estimate the parameters of the proposed function by solving a least-squares problem, or by cross-correlating the galaxy density map with the potential systematic maps \citep[see e.g.][]{ross2011ameliorating, ashley2012MNRAS,Ross17,2012ApJ...761...14H,delubac2016sdss, prakash2016sdss, Raichoor2017MNRAS.471.3955R, laurent2017clustering, Elvin18, 2018ApJ...863..110B}. The best fit model produces a \textit{selection mask (function)} or a set of \textit{photometric weights} that quantifies the systematic effects in the galaxy density fluctuation induced by the imaging pipeline, survey depth, and other observational attributes. The selection mask is then used to up-weight the observed galaxy density map to mitigate the systematic effects. The regression-based methods often assume a linear model (with linear or quadratic polynomial terms), and use all of the data to estimate the parameters of the given regression model; however, the assumption that the systematic effects are linear might not necessarily hold for strong contamination, e.g., close to the Galactic plane. \citet{2012ApJ...761...14H} analyzed photometric Luminous Red Galaxies in SDSS-III Data Release 8 and showed that the excess clustering due to the stellar contamination on large scales (e.g., roughly greater than twenty degrees) cannot be removed with a linear approximation. \citet{rossfNL} investigated the local non-Gaussianity ($f_{NL}$) using the BOSS Data Release 9 ``CMASS" sample of galaxies \citep{ahn2012ninth} and found that a robust cosmological measurement on very large scales is essentially limited by the systematic effects. Their analysis indicated that a more effective systematics correction is preferred relative to the selection mask based on the linear modeling of the stellar density contamination. Recently, \citet{Elvin18} developed a methodology based on $\chi^{2}$ statistics to rank the imaging maps based on their significance, and derived the selection mask by regressing against the significant maps.\\

Another promising, yet computationally expensive, approach injects artificial sources into real imaging in order to forward-model the galaxy survey selection mask introduced by real imaging systematics \citep[see e.g.][]{berge2013ultra,Balrog}. Rapid developments of multi-core processors and efficient compilers will pave the path for the application of these methods on big galaxy surveys.\\




In this paper, we develop a systematics mitigation method based on artificial neural networks. Our methodology models the galaxy density dependence on observational imaging attributes to construct the selection mask, without making any prior assumption of the linearity of the fitting model. Most importantly, this methodology is less prone to over-training and the resulting removal of the clustering signal by performing $k$-fold cross-validation (i.e., splitting the data into $k$ number of groups/partitions from which one constructs the training, validation, and the test sets) and dimensionality reduction through backward feature elimination~(i.e., removing redundant and irrelevant imaging attributes)\citep[see e.g.,][]{devijver1982pattern, john1994irrelevant, koller1996toward, kohavi1997wrappers, ramaswamy2001multiclass, guyon2003introduction}.  By permutation of the training, validation, and test sets, the selection mask for the entire footprint is constructed.  We apply our method on galaxies in the Legacy Surveys Data Release 7 (DR7) \citep{dey2018overview} that are chosen with the eBOSS-ELG color-magnitude selection criteria~\citep{Raichoor2017MNRAS.471.3955R} and compare its performance with that of the conventional, linear and quadratic polynomial regression methods. While the effect of mitigation on the data will be estimated qualitatively as well as quantitatively based on cross-correlating the observed galaxy density field and imaging maps, the data does not allow an absolute comparison to the unknown underlying cosmology. We therefore simulate two sets of 100 mock datasets, without and with the systematic effects that mimic those of DR7, apply various mitigation techniques in the same way we treat the real data, and test the resulting clustering signals against the ground truth.\\


This paper is organized as follows. Section \ref{sec:data} presents the imaging dataset from the Legacy Surveys DR7 used for our analysis. In Section \ref{sec:method}, we describe our method of Artificial Feed Forward Neural Network in detail as well as the conventional multivariate regression approaches. In this section, we also explain the angular clustering statistics employed to assess the level of systematic effects and the mitigation efficiency. We further describe the procedure of producing the survey mocks with and without simulated contaminations. In Section \ref{sec:results}, we present the results of mitigation for both DR7 and the mocks. Finally, we conclude with a summary of our findings and a discussion of the benefits of our methodology for future galaxy surveys in Section \ref{sec:conclusion}.\\
