%
%
\section{Methodology}\label{sec:method}
\subsection{Observed galaxy density}
In our methodology, we treat the mitigation of imaging systematics as a regression problem, in which we aim to model the relationship between the observed galaxy density (\textit{label}) and the imaging attributes (\textit{features}) that are the potential sources of the systematic error. Note that we do not include the positional information as \textit{input} features since we do not want the mitigation to fit the cosmological clustering pattern. The solution of the regression then would provide the predicted mean galaxy density (i.e., in the absence of clustering or shot-noise) solely based on the imaging attributes of that location. We use this predicted galaxy density as the \textit{survey} selection mask to be applied to any observed galaxy map in the attempt to eliminate the systematic effects and therefore isolate the cosmological fluctuation. Below we describe our procedure.\\


In this paper, we focus on the multiplicative systematic effects. The observed number of galaxies within pixel $i$ can be expressed in terms of the true number of galaxies $n_{i}$ and the contamination model $\mathcal{F}(\textbf{s}_{i})$ as 
\begin{equation}\label{eq:ngal_fs}
    n_{i}^{o}(\textbf{s}_{i}) = n_{i} \mathcal{F}(\textbf{s}_{i}), 
\end{equation}
where \textbf{s}$_{i}$ is a vector representing the imaging attributes \textbf{s} of pixel $i$, and the contamination model $\mathcal{F}(\textbf{s}_i)$ is an unknown function representing the systematic effects which could be either a linear, non-linear, or a more complex combination of the imaging attributes. Multiplicative systematics are associated with obscuration and area-loss due to foreground stellar density, Galactic extinction, etc. On the other hand, additive systematics are associated mostly with stellar contamination, as described in \citet{myers2007clustering, ross2011ameliorating, 2012ApJ...761...14H, prakash2016sdss, 2016MNRAS.455.4301C}. When averaged over many pixels, the effect of additive systematics can be absorbed into the constant term of the multiplicative model $\mathcal{F}$, assuming there is no correlation between the imaging maps and the true galaxy density field. The modeling of $\mathcal{F}(\textbf{s}_{i})$ can be approached by a wide variety of techniques, ranging from the traditional methods based on multivariate functions to non-parametric and non-linear models based on machine learning or deep learning such as random tree forests and neural networks \citep{breiman2001random, geurts2006extremely}.\\

The cosmological information is contained in the true overdensity that is given by
\begin{equation}
\delta_{i} = n_i/(f_{{\rm pix},i}\bar{n})-1,\label{eq:overden}
\end{equation}
accounting for the pixel completeness where $\bar{n}$ is the `true' average number of galaxies. Then,
\begin{equation}
n_{i}^{o}= f_{{\rm pix},i}\overline{n}~(1 + \delta_{i})~ \mathcal{F}(\textbf{s}_{i}).\label{eq:nobsi}
\end{equation}
This $n_{i}^{o}/f_{{\rm pix},i}$ is equivalent to the observed $ngal$ aforementioned.
Since we do not know the true average number density $\overline{n}$ of the data, we estimate $\bar{n}$ from the average of the observed galaxy field,
\begin{equation}\label{eq:nbar}
\hat{\overline{n}} = \frac{\displaystyle\sum_{i} n^{o}_{i}  }{\displaystyle\sum_{i} f_{{\rm pix},i}},
\end{equation}
and treat $\hat{\overline{n}} \equiv \overline{n}$.
Due to the finite volume of our sample, $\hat{\overline{n}} \neq \overline{n}$ even in the absence of systematic effects. This imposes the well-known integral constraint effect on any clustering analysis. We further ignore any systematic effect on $\hat{\overline{n}}$ due to the fact we use $n^{o}_{i}$; that is, Eq.~\ref{eq:nbar} converges to $\overline{n}$ only when $\sum_{i} f_{{\rm pix},i}= \sum_{i} f_{{\rm pix},i}\mathcal{F}_i$. 
In this sense we are modeling the relative systematic effect without necessarily determining the accurate `true' $\overline{n}$. We will use simulated results to test our methodology, and the analysis applied to the simulations with a limited footprint will be subject to the similar finite-volume and systematic effects on $\hat{\overline{n}}$, thus providing a fair comparison and means to catch any obvious problem with this approximation.\\

Finally, we define the normalized galaxy density per pixel $t_{i}$,
\begin{equation}\label{eq:nnbar}
    t_{i}(\textbf{s}_{i}) \equiv \frac{n_{i}^{o}(\textbf{s}_{i})}{f_{{\rm pix},i}\hat{\overline{n}}} = (1+\delta_{i}) ~ \mathcal{F}(\textbf{s}_{i}).
\end{equation}
With this definition, we can estimate the unknown contamination model $\hat{\mathcal{F}}$ (or selection mask) by modeling the dependence of $t_{i}$ on $\textbf{s}_{i}$. When averaged over many spatial positions, the cosmological fluctuations $\delta_i$ will be averaged out and therefore the observed $t_i$ averaged across many pixels with the same imaging attribute, should only be a function of \textbf{s} and return $\mathcal{F}$: %\mro{}{Please also check the notation throughout equations 3-7, since I think there are either some discrepancies or undefined s.}
\begin{equation}\label{eq:tfs}
    <t_{i}(\textbf{s}_{i})>_{i} \simeq <{\mathcal{F}}(\textbf{s}_{i})>_{i} = {\mathcal{F}}(\textbf{s}).
\end{equation}
The inverse of the selection mask which is equivalent to the photometric weights ($wt^{{\rm sys}}$) in other studies can therefore be used to remove the systematic effects from the observed galaxy number map (cf. Eq. \ref{eq:ngal_fs}),

\begin{align}\label{eq:wsys}
\hat{n}_i = \frac{n^o_i}{\hat{\mathcal{F}}}=n^o_i wt^{{\rm sys}}_{i}.
\end{align}
In the following we describe how we obtain $\hat{\mathcal{F}}$ using different regression approaches, e.g., neural networks and multivariate linear functions. From now on, the terms \textit{features} and \textit{label} associated with each data point refer to \textbf{s} and $t$ of each HEALPix pixel, respectively. 

\subsection{Mitigation with Neural Networks}\label{subsec:MethodNN}
We will apply {\it fully connected feed forward neural networks} in order to tackle our regression problem. Fig. \ref{fig:perceptron} illustrates a schematic diagram of a neuron, the building block of a neural network, which generates the output based on a linear combination of the inputs followed by a nonlinear transformation, the activation function $f$.\\

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figures/fig2-perceptron.pdf}
\caption{A schematic diagram of a single neuron with the activation function $f$. The neuron takes a set of inputs \textbf{x}=$(x_{1}, x_{2},...,x_{n})$, multiplies each of them by its associated weight \textbf{w}=$(w_{1}, w_{2},...,w_{n})$, and sums the weighted values and a thresh-hold (or a constant offset) which is called \textit{bias}, to form a pre-activation value, $z=\sum_{i=1}^{n}x_{i}w_{i} + bias$, which is a linear process. The neuron then transforms the pre-activation $z$ to the output using the activation function $f(z)$, which is where the nonlinear process can enter. }\label{fig:perceptron}
\end{figure}

Fig. \ref{fig:ffnn} illustrates the architecture of a fully connected feed forward neural network with the imaging attributes in the input layer, three hidden layers of six non-linear neurons in the middle, and a single neuron without any activation function in the output layer, as an example. The \textit{bias} neuron in each layer is shown in orange and is analogous to the intercept in linear regression. The output of the neural network will be an estimation of the contamination model $\hat{\mathcal{F}}$ (see Eq. \ref{eq:nnbar}). If we use the identity function as the activation function (e.g., $f(z)=z$), regardless of the number of hidden layers, the neural network is equivalent to a linear model. This means that our methodology is a generalization or an extension of the conventional linear mitigation methods. The modeling capabilities of neural networks depend on the number of hidden layers, type of non-linear activation function and the number of neurons in each hidden layer \citep[see e.g.][]{cybenko1989approximation,hornik1989multilayer,funahashi1989approximate, tamura1997capabilities, huang2003learning, lin2017does, rolnick2017power}.\\

We use the rectifier $f(z) = \text{max}(0, z)$ as the activation function for the hidden layer neurons, which alleviates the `vanishing gradient' problem
~\citep[see e.g.,][]{nair2010rectified,glorot2011deep,krizhevsky2012imagenet, dahl2013improving,montufar2014number}. \\

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{fig3-ffnn.pdf}
\caption{A schematic illustration of a fully connected feed forward neural network with the imaging attributes in the input layer, three hidden layers of six neurons in the middle, and a single neuron on the output layer, as an example. The blue-colored neurons have non-linear activation functions, while the red-colored neuron lacks any activation function. In reality, we employ the validation procedure to choose the best number of hidden layers while keeping the total number of hidden layer neurons fixed at 40 (i.e., approximately twice the number of imaging attributes in this study).}\label{fig:ffnn}
\end{figure}

We utilize  $k$-fold cross-validation with $k=5$ folds/sub-groups to train the parameters, tune the hyper-parameters, and to estimate the predictive performance of the neural network. As illustrated in Fig. \ref{fig:5fold}, we randomly split the entire pixel data (187,257 pixels) into five folds and construct the training, validation and test data sets out of these five folds; three folds are assigned to the training set, one fold is assigned to the validation set, and the remaining one fold is assigned to the test set. A specific assignment of the five folds to these three sets forms one `partition'. We construct five different partitions such that each fold is used once as test fold. This $k$-fold cross-validation scheme ensures that a test example is never used for training or tuning.\\

\begin{figure}
\centering
 \includegraphics[width=0.35\textwidth]{figures/fig4-5fold.pdf}
 \caption{A visualization of the five-fold permutations of data-split. The data is randomly split into five equal-size folds, and by permutation of the folds we construct five partitions of data. For each partition/permutation, three folds are assigned to the training set, one fold for the validation set, and the remaining one fold for the test set: therefore, 60\% training, 20\% validation, and 20\% test sets. Each column represents a partition. The test folds are shown in red, while the training and validation folds are shown in blue. The key points are : 1) The folds are not contiguous (in RA, DEC) as may be implied by this cartoon. 2) There is no overlap between the training, validation, and test folds within a partition. 3) One can reconstruct the entire data by merging the test folds from the five partitions.}
\label{fig:5fold}
\end{figure}
     
     
We standardize (i.e., renormalize) the label and features of the training, validation, and test folds using the mean and standard deviation of the label and features of the corresponding training fold (i.e., similar to Tab.~\ref{tab:meanstats}, but for the training set of each partition). We initialize the biases to zero and sample the weights of each layer randomly from a normal distribution whose variance is inversely proportional to the number of neurons of the previous layer \citep[see e.g.,][]{he2015delving}. Using the training fold, we utilize the adaptive gradient descent with momentum \citep[\textit{Adam},][]{kingma2014adam} to update the parameters of the neural network with batches of size $N_{{\rm batch}}$. Thus, the entire training set is split into $N_{{\rm batch}}$ batches and a gradient update is applied for each batch. One training epoch corresponds to processing the $N_{{\rm batch}}$ batches once \citep[for more details on the training procedure, we refer the reader to see e.g.,][]{ruder2016overview}. The hyper-parameters of \textit{Adam}, specifically the moments decay rates and the tolerance, are fixed as follows: $\beta_{1}$ = 0.9, $\beta_{2}$ = 0.999 and $\epsilon$ = 10$^{-8}$. The default learning rate of 0.001 will be tuned using the validation data.\\ 

The network is trained to minimize the following cost function:
\begin{equation}\label{eq:cost}
    \text{J} = \frac{1}{N_{{\rm batch}}}\sum_{i}^{N_{{\rm batch}}}f_{\rm pix, i}~[t_{i} - \hat{\mathcal{F}_{i}}]^{2} + \frac{\lambda}{2} ~ ||w||^{2} ,
\end{equation}   
%
where the first term is the Mean Squared Error (MSE) weighted with $f_{\rm pix,i}$, and the second term is the L2 regularization term, used to penalize higher weight magnitudes and a larger number of neurons~\citep{hoerl1970ridge}. The strength of the L2 penalty term is controlled via the regularization scale $\lambda$. The network is trained for a number of training epochs, $N_{{\rm epoch}}$, although to avoid unnecessary training, we implement the early stopping technique with the tolerance of 1.e-4 and patience of 10, i.e., the training terminates if the validation MSE does not improve more than the tolerance within the last 10 epochs.\\

\subsubsection{Backward feature elimination}
    
The input features are highly correlated as shown in Fig.~\ref{fig:eboss_dr7}, and therefore the 18 maps probably contain redundant information.
We apply \textit{backward feature elimination} (feature selection) to remove the redundant input features in order to reduce the noise in the prediction as well as to protect the cosmological information by avoiding too much freedom in modeling. We find that reducing the dimension of the input features, i.e., the imaging attributes, is an essential step to avoid over-fitting and regressing out the cosmological clustering.\\

We perform the feature selection for each partition separately. Initially, we train a linear model on all 18 input features with the following hyper-parameters: the initial learning rate of 0.001, batch-size of 1024, L2 regularization scale of zero, for 500 training epochs with early stopping. We record the validation MSE as a baseline criterion. Then we eliminate one input feature and train the linear model on the remaining 17 features. This trained linear model is applied to the validation set. The input feature whose removal has produced the highest decrease in validation MSE (i.e., the highest improvement in fitting) is permanently eliminated, leaving only 17 features for training. Note that if the feature contained useful information on the systematic effects, removing the feature would have made the fit worse. We repeat the regression using 17 input features, and so on, removing one feature at each iteration until either the validation MSE does not decrease relative to the baseline or all input features are removed. Fig. \ref{fig:dr7ablation} shows the result of the backward feature elimination procedure for the first partition of DR7, ranking the input features based on their importance from left to right. This result supports the trends seen in the exploratory analysis in \S~\ref{sec:data} which indicated strong linear correlations with the stellar density, Galactic extinction, and hydrogen column density in the data (see the correlation matrix in the bottom panel of Fig. \ref{fig:eboss_dr7}). The color gradient indicates the relative change in validation Root Mean Squared Error (RMSE) when that particular feature is removed with reference to the baseline. We note that the order of removal is not the same as, for example, the color gradient order of the attributes in the first iteration. We believe it is because, as we remove the redundant features, the relevant importance of the remaining input features changes due to the complex correlations between the removed features and the remaining ones. The remaining features for each of the five partitions are shown in Fig. \ref{fig:dr7ablation2}. The attributes $lnHI$ and $nstar$ are commonly identified as the most important features and then $ebv$, $seeing-g$, $skymag-g$, $skymag-z$, $exptime-r$, $exptime-z$, $mjd-z$ are commonly identified for all 5 partitions.\\

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/fig5-dr7ablation.pdf}
    \caption{Feature importance of DR7 based on backward feature elimination for the first partition, as an example. This process iteratively removes the feature whose removal produces the largest decrease in the validation RMSE (i.e., the greatest improvement in fitting) until no decrease is observed. After the first iteration of removal (the top row), the removal of $skymag-r$ decreased the validation RMSE the most and therefore $skymag-r$ is removed. In the second iteration (the second row), removing $exptime-g$ decreased the validation RMSE the most and therefore it is removed. However, in the ninth iteration, the removal of $mjd-z$ did not decrease the validation RMSE and therefore the feature selection stops here, passing the rightmost ten features to the neural network regression. As a result of the process, the importance increases from left to right, and the rightmost ten maps in the figure ($ebv$, $nstar$, $logHI$, $seeing-g$, $skymag-g$, $skymag-z$, $exptime-r$, $exptime-z$, $mjd-g$, $mjd-z$) are the ones that worsen the validation RMSE when being removed from the input layer.}
    \label{fig:dr7ablation}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/fig6-dr7ablation2.pdf}
    \caption{Important imaging maps identified by the backward feature elimination (\textit{feature selection}) procedure for the five partitions used for DR7. A darker color of a point within each partition represents a more important attribute identified by the feature selection procedure. Note that $nstar$ is identified as the most important attribute in all partitions, i.e., across the footprint.}
    \label{fig:dr7ablation2}
\end{figure}
 

\subsubsection{Hyper-parameter tuning, training, and testing}\label{subsubsec:hyperparam}

We train the hyper-parameters for each partition separately. At each training epoch and for each choice of hyper-parameters, we apply the trained network on the validation fold. We adjust the hyper-parameters accordingly such that the validation MSE is minimized. Our neural network has the following five hyper-parameters: number of hidden layers; number of training epochs $N_{{\rm epoch}}$; L2 regularization scale $\lambda$; batch size $N_{{\rm batch}}$; Adam's learning rate. We tune one hyper-parameter at a time. To find the optimal learning rate, we monitor the behavior of the cost function during training. We observe that a learning rate of 0.001 leads to a smoothly decreasing cost function vs. training epochs. We train the network for up to $N_{\rm epoch} = 500$ epochs although we implement the early stopping technique with the inertia (or patience) of 10 and the tolerance of 1.e-4: this means the training will be stopped if none of the last 10 epochs achieved a smaller relative error reduction with respect to the minimum validation error, within the tolerance. For the number of hidden layers, we try the following architectures, in which the total number of hidden neurons is fixed at 40 (i.e. roughly twice the number of the features) except for the linear model:\\~\\
%
%
$[0]$  : no hidden layers \\
$[40]$ : one hidden layer of 40 neurons \\
$[20, 20]$ : two hidden layers of 20 neurons on each \\
$[20, 10, 10]$ : three hidden layers of 20, 10 and 10 neurons \\
$[10, 10, 10, 10]$ : four hidden layers of 10 neurons\\~\\
After finding the best number of layers, we proceed to tune $\lambda$ by trying powers of 10, e.g., 0.001, 0.01, ..., 1, ..., 1000. Finally, we adjust $N_{\rm batch}$ by trying powers of 2, e.g., 128, 256, ... , 4096. The optimal set of the hyper-parameters for each partition is summarized in Tab. \ref{tab:hparams}.\\

\begin{table}
    \centering
    \caption{The best hyper-parameters for each partition of DR7.}
    \label{tab:hparams}
    \begin{tabular}{lccr} % four columns, alignment for each
        \hline
        %\hline
         & number of layers & $\lambda$ & $N_{\rm batch}$ \\
        \hline
        Partition 1 & [20, 20] & 0.001 & 4096\\
        Partition 2 & [20, 10, 10] & 0.001 & 512\\
        Partition 3 & [20, 10, 10] & 0.001 & 1024 \\
        Partition 4 & [20, 10, 10] & 0.001 & 512 \\
        Partition 5 &  [20, 10, 10] & 0.001 & 2048\\
        %\hline
        %\hline
    \end{tabular}
\end{table}

Once the grid search procedure identifies the best performing hyper-parameters out of the predefined ranges introduced in Section \ref{subsubsec:hyperparam}, the network is trained with these hyper-parameters for 10 independent runs, each one with a different initialization of the weights and biases, and then applied on the test set. We compute the median of the predicted test label from the 10 runs and aggregate the results over the 5 different partitions to construct the map of the predicted label ($\hat{\mathcal{F}}$) for the entire footprint. For our default method, the backward feature elimination is conducted for each partition and reduces the number of input features before the hyper-parameter training step, as illustrated in Fig.~\ref{fig:pipeline}.  This process is performed for each partition separately, each partition using a different fold as the test set, until the entire footprint is covered through the 5 test folds. The flow of the feature selection, hyper-parameter tuning, and testing is summarized in Fig. \ref{fig:pipeline}.\\

\begin{figure*}
         \centering
         \includegraphics[ width=0.8\textwidth]{figures/fig7-pipeline.pdf}
         \caption{A flow-chart of the backward feature elimination and hyper-parameter tuning for each partition. This entire process is performed five times for each of the five partitions/permutations such that the entire footprint is covered by aggregating the different testing folds. }
         \label{fig:pipeline}
\end{figure*}

% multivariate
\subsection{Mitigation with Multivariate Linear Functions}
We use linear and quadratic polynomial functions to model the normalized galaxy density dependence on the imaging attributes (Eq.~\ref{eq:tfs}), as the benchmark approaches to be compared with the neural network. Unlike the proposed neural network method, no regularization or dimensionality reduction is performed, and all data are used to train the parameters of the regression models. Despite a lack of any deliberate machinery against over-fitting, we note that over-fitting is less likely to be an issue for this method since the size of the data is much greater than the number of the fitting parameters. Nevertheless, we tried splitting the sample into 60\% of the training data to derive the best fit linear coefficients and 20\% of the test set (i.e., the same training and test sample size to have a fair comparison with the neural network) to apply the derived coefficients and permuted five times until the test set covers the entire footprint. We find that such a split does not change the results of the linear regression. On the other hand, the limited flexibility of its parameterized form could be a weakness of this method and we believe it is responsible for the differences that the neural network method makes in the comparison presented in Section \ref{sec:mitigateDR7}.\\

The contamination model from Eqs.~\ref{eq:nnbar} and ~\ref{eq:tfs} can be estimated via a multivariate linear function in terms of the standardized imaging attributes (\textbf{s}) as,

\begin{equation}\label{eq:multvar}
\hat{\mathcal{F}}(\textbf{s}|b_{0},\alpha) = b_{0} + \sum_{m=1}^{M}\sum_{k=1}^{18} \alpha_{mk} (\frac{s_{k}-\overline{s_{k}}}{\sigma_{k}})^{m} ,
\end{equation}
where $M$ is the maximum power law index equal to 1 and 2 for the linear and quadratic polynomial model, respectively; the constants $\overline{s_{k}}$ and $\sigma_{k}$ are respectively the mean and standard deviation of the k'th imaging quantity, $s_{k}$ (cf. Tab. \ref{tab:meanstats}). The parameters $b_{0}$ and $\alpha_{mk}$ are the intercept and the corresponding coefficients for each term, respectively, which are tuned by minimizing the weighted sum of the squared errors. The output of the regression is applied as the selection mask on the observed galaxy density to eliminate the systematic effects (see Eq., \ref{eq:wsys}).\\

\subsection{Angular Clustering Statistics}\label{subsec:ang_clustering}
\subsubsection{One point statistics}
In the absence of the systematic effects, the galaxy density field should be statistically independent from the imaging attributes and will only depend on the cosmological fluctuations while an individual dataset/mock will be subject to chance correlations within the statistical error. When averaged over many spatial positions, the cosmological fluctuations will be averaged out and therefore the average density should be equal to the mean density once the survey footprint is accounted for. A deviation from the mean density as a function of the imaging attributes, therefore, is indicative of the average dependence of the observed galaxy density on the corresponding imaging attributes. To assess the level of the contamination in the data, we compute the histogram of the spatially averaged galaxy density vs the imaging attributes. For each of the system attribute, $s_k$, we prepare 20 bins. For each bin of $s_{k}$, we have:

\begin{equation}\label{eq:nnbar_stat}
    \frac{\overline{n}(s_{k})}{\overline{n}_{tot}} \equiv \frac{1}{\hat{\overline{n}}}~ \frac{\displaystyle\sum_{s_{k}\leq s_{k,i}<s_{k}+\Delta s_{k}} n^{o}_{i}}{\displaystyle\sum_{s_{k}\leq s_{k,i}<s_{k}+\Delta s_{k}} f_{\rm pix, i}},
\end{equation}
where the indices $i$ and $k$ respectively represent the pixel index, and the systematic index; $\Delta s_{k}$ is the bin width arranged for different $s_k$ bins such that each bin contains almost the same amount of effective area, in an attempt to suppress fluctuations in $\overline{n}(s_{k})/\overline{n}_{tot}$ due to small number statistics. We estimate the error bars using the Jackknife resampling of 20 non-contiguous subsamples of pixels within each imaging attribute bin ~\citep[see e.g.][]{ross2011ameliorating}:


\begin{equation}\label{eq:error_jack}
    \sigma^{2}_{{\rm Jack}}(s_{k}) = \frac{19}{20}\sum_{j=1}^{20} \left[    \frac{\overline{n}(s_{k})}{\overline{n}_{tot}} -     \frac{\overline{n}_{j}(s_{k})}{\overline{n}_{tot}}\right]^{2}  ,
\end{equation}
 where $\overline{n}_{j}(s_{k})/\overline{n}_{tot}$ is computed over the entire sample when the $j$'th Jackknife region is excluded.
As a result of the adjusted $\Delta s_{k}$, the level of $\sigma^{2}_{{\rm Jack}}(s_k)$ is almost the same for all $s_k$. After mitigation of the systematic effects, one expects that the corrected density field is independent of the imaging attributes, i.e., $\overline{n}(s_{k})/\overline{n}_{tot}$ being consistent with unity.


\subsubsection{Two-point clustering statistics}
The two-point clustering statistic measures the spatial correlation of the galaxy density and has been the main statistic for extracting the cosmological information from galaxy surveys. We use the angular auto and cross two-point clustering statistics of the galaxy density field as well as of the imaging attributes to estimate the impact of the potential systematics on the cosmological clustering signal and to examine the effectiveness of the mitigation techniques tested in this paper. For pixel $i$, we calculate the galaxy overdensity $\delta_{i}$ using Eq.~\ref{eq:overden} and the fluctuation of a given imaging attribute $\delta^{s}_{i}$ as
\begin{align}\label{eq:delta}
    \hat{\delta}^{s}_{i} &= \frac{s_{i}}{\hat{\overline{s}}} - 1 ,
\end{align}
where $\hat{\overline{s}}$ is the mean of each imaging attribute weighted with $f_{{\rm pix},i}$,
\begin{align}\label{eq:deltasysbar}
\hat{\overline{s}} = \frac{\sum_i f_{{\rm pix},i} s_i}{\sum_i f_{{\rm pix},i}}
\end{align}
following \citet{ross2011ameliorating}.\\

By definition, Eqs.~\ref{eq:nbar} and \ref{eq:delta} ensure that the following integral of the observed quantity over the entire footprint vanishes:
\begin{align}\label{eq:ic}
    \sum_{i} \hat{\delta}_{i}~f_{{\rm pix},i} = 0,
\end{align}
for both the galaxy as well as imaging attribute fluctuations. We utilize both the angular correlation function and angular power spectrum to extract the cosmological information from the galaxy density field. While our mitigation efficiency is evaluated based on the angular power spectrum, we also inspect the angular correlation function to make sure that the systematics are mitigated in that estimator as well, since both estimators are commonly used in the clustering analysis and they are complementary to each other given the limited range of data.


\begin{itemize}
    \item Angular Correlation Function: we employ the HEALPix-based estimator to compute the angular correlation function which, for a separation angle $\theta$, is defined as \citep[see e.g.][]{scranton2002analysis,ross2011ameliorating}
    \begin{align}
        \omega^{p,q} (\theta) = \frac{\displaystyle\sum_{ij} \hat{\delta}^{p}_{i} \hat{\delta}^{q}_{j} \Theta_{ij}(\theta) f_{\rm pix, i}f_{\rm pix, j}}{\displaystyle\sum_{ij} \Theta_{ij}(\theta) f_{\rm pix, i}f_{\rm pix, j}},\label{eq:ancor}
    \end{align}
    where $p=q$ gives an auto correlation function estimator, $p\neq q$ gives a cross correlation function estimator, and $\Theta_{ij}$ is one when two pixels $i$ and $j$ are separated from each other within $\theta$ and $\theta+\Delta\theta$, or zero otherwise. Note that our estimator weighs each pixel overdensity with $f_{{\rm pix}, i}$ since the pixels with a greater complete area coverage should have a higher signal to noise. Such weight is straightforwardly corrected by the denominator in Eq.~\ref{eq:ancor} unlike its conjugate estimator (i.e., the power spectrum). Since our overdensity map resolution is limited by the pixel size, we set the $\Delta\theta$ to be the resolution of a pixel ($\sim$ 0.23 deg). \\


    \item Angular Power Spectrum : one can conveniently expand a coordinate on the surface of a sphere in terms of spherical harmonics or, if azimuthally symmetric, Legendre polynomials. We define the following estimator for expanding the galaxy overdensity:
    \begin{equation}
        \hat{\delta}_{i} = \sum_{\ell=0}^{\ell_{{\rm max}}}\sum_{m=-\ell}^{\ell} a_{\ell m} Y_{\ell m}(\theta_{i}, \phi_{i}),
    \end{equation}
    where $\theta, \phi$ represent the polar and azimuthal angular coordinates of pixel \textit{i}, respectively. The cutoff at $\ell=\ell_{{\rm max}}$ assumes that the signal power is not significant for modes $\ell>\ell_{{\rm max}}$. We define the following spherical harmonic (SH) transform estimator of overdensity ($\hat{\delta}$) over the total number of non-empty pixels $N_{{\rm pix}}$:
   
    \begin{equation}
        \hat{a}_{\ell m} = \frac{4\pi}{N_{{\rm pix}}} \sum_{i=1}^{N_{{\rm pix}}}  \hat{\delta}_{i}~f_{\rm pix, i}~ Y^{*}_{\ell m}(\theta_{i}, \phi_{i}),
    \end{equation}
    where $^{*}$ represents the complex conjugate, and we again down-weight the overdensity in pixel \textit{i} by the completeness ($f_{\rm pix, i}$). Due to the survey window function implicit in the sum over the non-empty pixels and explicit in $f_{\rm pix, i}$, our estimator would not return unbiased estimates of the SH coefficients, unless the window function effect is corrected for, and also the expected orthogonalities between different SH modes would not hold. Nevertheless, we define the angular power spectrum estimator as the average of the magnitude of SH coefficients over $m$:
    \begin{equation}\label{eq:pusedocell}
        \hat{C}^{p,q}_{\ell} = \frac{1}{2\ell +1} \sum_{m=-\ell}^{\ell} \hat{a}^{p}_{\ell m} \hat{a}^{q*}_{\ell m},
    \end{equation}
    where $p=q$ gives an auto power spectrum, $p\neq q$ gives a cross power spectrum between the galaxy density and the imaging attributes. In order to compute the angular power spectrum, $C_{\ell}$, we make use of the ANAFAST function from HEALPix \citep{gorski2005healpix} with the third order iteration of the quadrature to increase the accuracy\footnote{We refer the reader to \url{https://healpix.sourceforge.io/pdf/subroutines.pdf}, page 104.}. 
    Unlike in the angular correlation function, we do not attempt to correct for the survey window function/survey mask effect in the angular power spectrum both for DR7 and the mocks. We rather calculate the window effect on the theoretical models of power spectrum in Appendix \ref{app:windowfunction}. For the mock test, we use the angular power spectrum observed in the mocks without the contamination model, i.e., the `Null' case, as our baseline to compare with different mitigation methods.\\ 
\end{itemize}
%
%
We use the Jackknife resampling technique with 20 equal-area contiguous regions, as shown in Fig. \ref{fig:jackknifes}, to estimate the error-bars on $\omega (\theta)$ and $C_{\ell}$ (see Eq. \ref{eq:error_jack}).\footnote{We use the mocks without imaging systematics (null mocks in Section \ref{subsec:surveymocks}) to compare the errors from the Jackknife subsamples of one mock with the errors among 100 full  DECaLS-like mock footprints, and we find that the former is greater than the latter on ell $<$ $\sim$ 10 by a factor of 2-3, possibly due to the dispersion in the survey window function of the Jackknife samples. For a real survey the observational condition also varies across the footprint. Therefore, with Jackknife errors, the significance of any improvement on systematics treatment will be conservatively assessed.} For both the mock and real datasets, we also utilize the cross power spectra between the galaxy density and various imaging maps to evaluate the performance of the mitigation. In order to estimate the significance of the contamination in $\hat{C}^{g,g}_{\ell}$ (or $\omega^{g,g} (\theta)$) before and after mitigation, we calculate $[\hat{C}^{g,s_k}_{\ell}]^2/\hat{C}^{s_k,s_k}_{\ell}$ (or $[\omega^{g,s_k}(\theta)]^2/\omega^{s_k,s_k}(\theta)$) as a proxy.~\footnote{These quantities would be the true level of contamination to $\hat{C}^{g,g}$ if the contamination model is linear and systematics are independent of one another~\citep{ashley2012MNRAS,2012ApJ...761...14H}.}
\begin{figure}
        \centering
        \includegraphics[width=0.4\textwidth]{figures/fig8-jackknifes.pdf}
        \caption{Twenty equal-area contiguous regions used to estimate the Jackknife errorbars for the 2D clustering statistics.}
        \label{fig:jackknifes}
\end{figure}
    

\subsection{Survey Mocks}\label{subsec:surveymocks}
Imaging systematics tend to affect the clustering signal mainly on large scales \citep{myers2007clustering, huterer2013calibration} and the distribution of galaxies on large scales at moderately low redshift can be well-approximated by a log-normal distribution~\citep{1991MNRAS.248....1C}. We therefore believe that log-normal mocks would be sufficient for the purpose of validating our systematic mitigation techniques. We use the \textit{Nbodykit} package \citep{hand2017nbodykit} to generate one hundred log-normal cubic mocks with the box-side length of $5274 \ihMpc$ and $1024^3$ mesh cells, with the input power spectrum matched to the linear power spectrum  at $z=0.85$ based on the \textit{Planck 2015} cosmology~\citep{ade2016planck} (i.e., flat $\Lambda$CDM with $\Omega_m=0.3089 \pm 0.0062$, $H_{0}=67.74 \pm 0.46$, $\sigma_8 = 0.8159 \pm 0.0086$), with the galaxy bias of 1.5 and the volume density of $1.947\mathrm{e}{-4}\trihMpc$ \citep[see e.g.][]{Raichoor2017MNRAS.471.3955R}. Then, we use the \textit{make\_survey} package \citep{white2013mock} to sub-sample the mock galaxies based on the NGC eBOSS ELG redshift distribution in \citet{Raichoor2017MNRAS.471.3955R} with the redshift cut of  0.55 $<$ z $<$ 1.5 and to transform the cubic mocks into survey-like mocks. We do not include redshift-space distortions (RSD) in the mocks as we believe that the systematics mitigation efficiency does not depend on  the presence of RSD.\\

The survey mocks are then projected onto the two-dimensional sky using HEALPix and overlaid on the NGC footprint of DR7 to be assigned with the DR7 imaging attributes. Fig. \ref{fig:mock_on_dr} illustrates the resulting projection of a simulated survey mock and DR7. Note that the mock footprint (89,672 pixels) is smaller than DR7 (187,257 pixels) almost by a factor of 2. We only use the pixels of the mock that have the DR7 imaging attributes available. The holes (e.g., RA and DEC around 200 and 5 deg) are the pixels that do not have the imaging attributes from the real data. In order to account for the mock survey footprint, we distribute 2,500 random points per deg$^{2}$ within the mock footprint and derive the completeness map for the mocks.


\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/fig9-mockondr.pdf}
    \caption{The projection of the mock footprint (blue) onto the North Galactic Cap of the DR7 footprint (gray). With this projection, the imaging attributes from the real data are assigned to the mocks.}
    \label{fig:mock_on_dr}
\end{figure}


\subsubsection{Null Mocks}
Our goal is to develop a systematics treatment methodology that maximally removes the systematic effects while minimally removes the true cosmological signal. The two aspects may not be simultaneously accomplished, in a way that depends on the signal, noise, and the correlation between the imaging maps and the true galaxy density. In our paper we choose to prioritize losing minimal cosmological information over maximally removing systematics. One way of ensuring this is to check if the mitigation method returns the true clustering in the presence of contaminations, which will be tested using the contaminated mocks. Another way is to check if the mitigation method correctly makes a null operation on the clustering in the absence of contaminations, returning $\hat{\mathcal{F}}\simeq 1$. To this end, we utilize the 2D projected mocks without introducing any modulation due to imaging attributes in the galaxy density fields. Henceforth, we call this set of simulations, \textit{null} mocks.\\

Fig.~\ref{fig:ngal_hist} shows the pixel distribution histograms of the number of galaxies per pixel of the mocks in comparison to that of DR7 on the common footprint. The average $ngal=7.0/{\rm pixel}$ of the null mocks is smaller than $ngal=13.3/{\rm pixel}$ of the DR7 data (even after accounting for the 5\% loss due to tiling completeness and 27\% loss due to the redshift range, as stated in \citet{Raichoor2017MNRAS.471.3955R}). We believe that the difference in $ngal$ is due to the different \textit{clean photometry} criteria applied to the ELG selection in \citet{Raichoor2017MNRAS.471.3955R} and to the targets in this paper. The standard deviation of $ngal$ of the null mocks is 3.0, which is smaller than 4.6 of the DR7 ELGs.

\subsubsection{Contaminated Mocks}
We modulate the mock galaxy density fields using imaging attributes of DR7 and generate the contaminated mocks with additional random noise. The modulation is done based on the best fit coefficients of the imaging attributes and their covariances for $\mathcal{F}$ (Eq.~\ref{eq:nnbar}) that we derived from DR7 using our fiducial linear regression model.\\


\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/fig10-ngalhist.pdf}
    \caption{Histogram of the number of galaxies per pixel for DR7 (hatched grey), null (solid blue), and contaminated (dashed red) mocks. All distributions are corrected for the pixel completeness $f_{\rm pix}$. The DR7 distribution is scaled down to account for 5\% tiling completeness, and 27\% to 0.55 < reliable redshift <1.5. The residual difference between the mocks and  DR7 shown here might be due to differences between the \textit{clean photometry} criteria applied to eBOSS target selection and those we apply to DR7.}
    \label{fig:ngal_hist}
\end{figure}

In detail, we pick the 10 imaging attributes, i.e., $EBV$, $nstar$, $lnHI$, $seeing-g$, $skymag-g$, $skymag-z$, $exptime-r$, $exptime-z$, $mjd-g$, $mjd-z$ of DR7, which were selected from the feature selection procedure on one of the partitions, and modulate the mock density field $n$ with $\mathcal{F}$ that is derived from random deviates of the imaging attribute coefficients while accounting for their covariances. Since the measured covariance of such quantities includes both the cosmological fluctuation and the fluctuations due to the imaging attributes, we rescale the measured covariance matrix of the systematics such that the random fluctuation in $ngal$ per pixel due to contamination is at a similar level to the cosmological fluctuation from the null mocks. As a result of the random fluctuation in the contamination model we introduced, some of the pixels will be assigned a negative galaxy number. We drop these pixels from our sample. This removes 3.1\% of the mock footprint, reducing our mock footprint size from 89,672 to 86,875 pixels. We then introduce the Poisson process, i.e., another random variation step, to ensure the modulated galaxy number per pixel is an integer. These two random variation processes increase the noise in the mock datasets such that the standard deviation of $ngal$ of the contaminated mocks ($=4.4$) is almost the same as that of DR7, despite the different average $ngal$ (see Fig.~\ref{fig:ngal_hist}).\\



Therefore our mock contamination is simpler than DR7 in that we adopted a linear model, which is chosen purposely since we do not want to give a priori advantage to our neural network method and also since all methods are capable of reproducing the linear model. Meanwhile, this setup is more challenging than the DR7 data since the mitigation is conducted in the presence of a greater level of noise. Note that, while we included only 10 dominant imaging attributes in the contamination, the remaining attributes in the DR7 data are correlated with these 10 attributes and therefore with the modulated galaxy density. All of the mitigation methods in the following mock test will be challenged to deal with such indirect correlations among the 18 attributes. Note that the effect of the footprint, i.e., the survey window effect, is the same for both the null mocks and contaminated mocks since we chose to apply the selection function on the galaxies while leaving the randoms intact. Therefore the null mocks serve as the baseline for estimating the level of systematics in the contaminated mocks.



