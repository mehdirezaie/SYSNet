\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\citation{efstathiou1988analysis,fisher1993power,smoot1992structure,mather1994measurement,riess1998observational,perlmutter1999measurements,ata2017clustering,BOSSfinal,jones2018measuring,akrami2018planck,Elvin18}
\citation{peebles1973statistical,kaiser1987clustering,mukhanov1992theory,hamilton1998linear,eisenstein1998cosmic,seo2003probing,eisenstein2005dark,sanchez2008best,dalal2008imprints}
\citation{york2000sloan,colless20012df,drinkwater2010wigglez}
\citation{aghamousa2016desi}
\newlabel{firstpage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{ivezic2008lsst,LSSTObservingStrategyWhitePaper}
\citation{myers2007clustering,thomas2011angular,thomas2011excess,ross2011ameliorating,ashley2012MNRAS,2012ApJ...761...14H,huterer2013calibration,pullen2013systematic}
\citation{rybicki1992interpolation,tegmark1997measure,tegmark1998measuring,slosar2004exact,ho2008correlation,pullen2013systematic,leistedt2013estimating,leistedt2014exploiting}
\citation{leistedt2013estimating}
\citation{leistedt2014exploiting}
\citation{elsner2015unbiased}
\citation{kalus2016unbiased}
\citation{kalus2018map}
\citation{ross2011ameliorating,ashley2012MNRAS,Ross17,2012ApJ...761...14H,delubac2016sdss,prakash2016sdss,Raichoor2017MNRAS.471.3955R,laurent2017clustering,Elvin18}
\citation{2012ApJ...761...14H}
\citation{rossfNL}
\citation{ahn2012ninth}
\citation{Elvin18}
\citation{berge2013ultra,Balrog}
\citation{devijver1982pattern,john1994irrelevant,koller1996toward,kohavi1997wrappers,ramaswamy2001multiclass,guyon2003introduction}
\citation{dey2018overview}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{dey2018overview}
\citation{wright2010wide}
\citation{lang2016tractor,aghamousa2016desi}
\citation{zou2017project}
\citation{dey2018overview}
\citation{dark2005dark}
\citation{gorski2005healpix}
\citation{LeistedtMap}
\citation{schlegel1998maps}
\citation{brown2018gaia}
\citation{bekhti2016hi4pi}
\citation{schlafly2011measuring}
\citation{brown2018gaia}
\citation{ashley2012MNRAS}
\@writefile{toc}{\contentsline {section}{\numberline {2}Legacy Surveys DR7}{3}{section.2}}
\newlabel{sec:data}{{2}{3}{Legacy Surveys DR7}{section.2}{}}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{dey2018overview}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The mean and sample standard deviation of the DECaLS DR7 imaging attributes used in this paper.}}{4}{table.1}}
\newlabel{tab:meanstats}{{1}{4}{The mean and sample standard deviation of the DECaLS DR7 imaging attributes used in this paper}{table.1}{}}
\newlabel{eq:depth_cuts}{{1}{4}{Legacy Surveys DR7}{equation.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The Northern Galactic Cap color-magnitude selection of the eBOSS Emission Line Galaxies \citep  {Raichoor2017MNRAS.471.3955R}. We enforce the same selection for the entire sky. Note that our selection is slightly different from \citet  {Raichoor2017MNRAS.471.3955R} in the clean photometry criteria as explained in the main text.}}{4}{table.2}}
\newlabel{tab:ts}{{2}{4}{The Northern Galactic Cap color-magnitude selection of the eBOSS Emission Line Galaxies \citep {Raichoor2017MNRAS.471.3955R}. We enforce the same selection for the entire sky. Note that our selection is slightly different from \citet {Raichoor2017MNRAS.471.3955R} in the clean photometry criteria as explained in the main text}{table.2}{}}
\newlabel{eq:pcc}{{2}{4}{Legacy Surveys DR7}{equation.2}{}}
\citation{breiman2001random,geurts2006extremely}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textit  {Top panel}: the pixelated density map of eBOSS-like ELGs from the DECaLS DR7 after correcting for the completeness of each pixel and masking based on the survey depth and completeness cuts. The solid red curve represents the Galactic plane. This figure is generated by the code described in \url  {https://nbviewer.jupyter.org/github/desihub/desiutil/blob/master/doc/nb/SkyMapExamples.ipynb}. \textit  {Bottom panel}: the color-coded Pearson correlation matrix between each pair of the imaging attributes from the DECaLS DR7.}}{5}{figure.1}}
\newlabel{fig:eboss_dr7}{{1}{5}{\textit {Top panel}: the pixelated density map of eBOSS-like ELGs from the DECaLS DR7 after correcting for the completeness of each pixel and masking based on the survey depth and completeness cuts. The solid red curve represents the Galactic plane. This figure is generated by the code described in \url {https://nbviewer.jupyter.org/github/desihub/desiutil/blob/master/doc/nb/SkyMapExamples.ipynb}. \textit {Bottom panel}: the color-coded Pearson correlation matrix between each pair of the imaging attributes from the DECaLS DR7}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{5}{section.3}}
\newlabel{sec:method}{{3}{5}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observed galaxy density}{5}{subsection.3.1}}
\newlabel{eq:ngal_fs}{{3}{5}{Observed galaxy density}{equation.3}{}}
\newlabel{eq:overden}{{4}{5}{Observed galaxy density}{equation.4}{}}
\citation{cybenko1989approximation,hornik1989multilayer,funahashi1989approximate,tamura1997capabilities,huang2003learning,lin2017does,rolnick2017power}
\citation{nair2010rectified,glorot2011deep,krizhevsky2012imagenet,dahl2013improving,montufar2014number}
\citation{he2015delving}
\citation{kingma2014adam}
\citation{ruder2016overview}
\newlabel{eq:nobsi}{{5}{6}{Observed galaxy density}{equation.5}{}}
\newlabel{eq:nbar}{{6}{6}{Observed galaxy density}{equation.6}{}}
\newlabel{eq:nnbar}{{7}{6}{Observed galaxy density}{equation.7}{}}
\newlabel{eq:tfs}{{8}{6}{Observed galaxy density}{equation.8}{}}
\newlabel{eq:wsys}{{9}{6}{Observed galaxy density}{equation.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Mitigation with Neural Networks}{6}{subsection.3.2}}
\newlabel{subsec:MethodNN}{{3.2}{6}{Mitigation with Neural Networks}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A schematic diagram of a single neuron with the activation function $f$. The neuron takes a set of inputs \textbf  {x}=$(x_{1}, x_{2},...,x_{n})$, multiplies each of them by its associated weight \textbf  {w}=$(w_{1}, w_{2},...,w_{n})$, and sums the weighted values and a thresh-hold (or a constant offset) which is called \textit  {bias}, to form a pre-activation value, $z=\DOTSI \sumop \slimits@ _{i=1}^{n}x_{i}w_{i} + bias$, which is a linear process. The neuron then transforms the pre-activation $z$ to the output using the activation function $f(z)$, which is where the nonlinear process can enter. }}{6}{figure.2}}
\newlabel{fig:perceptron}{{2}{6}{A schematic diagram of a single neuron with the activation function $f$. The neuron takes a set of inputs \textbf {x}=$(x_{1}, x_{2},...,x_{n})$, multiplies each of them by its associated weight \textbf {w}=$(w_{1}, w_{2},...,w_{n})$, and sums the weighted values and a thresh-hold (or a constant offset) which is called \textit {bias}, to form a pre-activation value, $z=\sum _{i=1}^{n}x_{i}w_{i} + bias$, which is a linear process. The neuron then transforms the pre-activation $z$ to the output using the activation function $f(z)$, which is where the nonlinear process can enter}{figure.2}{}}
\citation{hoerl1970ridge}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A schematic illustration of a fully connected feed forward neural network with the imaging attributes in the input layer, three hidden layers of six neurons in the middle, and a single neuron on the output layer, as an example. The blue colored neurons have non-linear activation functions, while the red colored neuron lacks any activation function. In reality, we employ the validation procedure to choose the best number of hidden layers while keeping the total number of hidden layer neurons fixed at 40 (i.e., approximately twice the number of imaging attributes in this study).}}{7}{figure.3}}
\newlabel{fig:ffnn}{{3}{7}{A schematic illustration of a fully connected feed forward neural network with the imaging attributes in the input layer, three hidden layers of six neurons in the middle, and a single neuron on the output layer, as an example. The blue colored neurons have non-linear activation functions, while the red colored neuron lacks any activation function. In reality, we employ the validation procedure to choose the best number of hidden layers while keeping the total number of hidden layer neurons fixed at 40 (i.e., approximately twice the number of imaging attributes in this study)}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A visualization of the five-fold permutations of data-split. The data is randomly split into five equal-size folds, and by permutation of the folds we construct five partitions of data. For each partition/permutation, three folds are assigned to the training set, one fold for the validation set, and the remaining one fold for the test set: therefore, 60\% training, 20\% validation, and 20\% test sets. Each column represents a partition. The test folds are shown in red, while the training and validation folds are shown in blue. The key points are : 1) The folds are not contiguous (in RA, DEC) as may be implied by this cartoon. 2) There is no overlap between the training, validation, and test folds within a partition. 3) One can reconstruct the entire data by merging the test folds from the five partitions.}}{7}{figure.4}}
\newlabel{fig:5fold}{{4}{7}{A visualization of the five-fold permutations of data-split. The data is randomly split into five equal-size folds, and by permutation of the folds we construct five partitions of data. For each partition/permutation, three folds are assigned to the training set, one fold for the validation set, and the remaining one fold for the test set: therefore, 60\% training, 20\% validation, and 20\% test sets. Each column represents a partition. The test folds are shown in red, while the training and validation folds are shown in blue. The key points are : 1) The folds are not contiguous (in RA, DEC) as may be implied by this cartoon. 2) There is no overlap between the training, validation, and test folds within a partition. 3) One can reconstruct the entire data by merging the test folds from the five partitions}{figure.4}{}}
\newlabel{eq:cost}{{10}{7}{Mitigation with Neural Networks}{equation.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Backward feature elimination}{7}{subsubsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Feature importance based on backward feature elimination for the first partition, as an example. This process iteratively removes the feature whose removal produces the largest decrease in the validation RMSE (i.e., the greatest improvement in fitting) until no decrease is observed. After the first iteration of removal (the top row), the removal of $skymag-r$ decreased the validation RMSE the most and therefore $skymag-r$ is removed. In the second iteration (the second row), removing $exptime-g$ decreased the validation RMSE the most and therefore it is removed. However, in the ninth iteration, the removal of $mjd-z$ did not decrease the validation RMSE and therefore the feature selection stops here, passing the rightmost ten features to the neural network regression. As a result of the process, the importance increases from left to right, and the rightmost ten maps in the figure ($ebv$, $nstar$, $logHI$, $seeing-g$, $skymag-g$, $skymag-z$, $exptime-r$, $exptime-z$, $mjd-g$, $mjd-z$) are the ones that worsen the validation RMSE when being removed from the input layer.}}{8}{figure.5}}
\newlabel{fig:dr7ablation}{{5}{8}{Feature importance based on backward feature elimination for the first partition, as an example. This process iteratively removes the feature whose removal produces the largest decrease in the validation RMSE (i.e., the greatest improvement in fitting) until no decrease is observed. After the first iteration of removal (the top row), the removal of $skymag-r$ decreased the validation RMSE the most and therefore $skymag-r$ is removed. In the second iteration (the second row), removing $exptime-g$ decreased the validation RMSE the most and therefore it is removed. However, in the ninth iteration, the removal of $mjd-z$ did not decrease the validation RMSE and therefore the feature selection stops here, passing the rightmost ten features to the neural network regression. As a result of the process, the importance increases from left to right, and the rightmost ten maps in the figure ($ebv$, $nstar$, $logHI$, $seeing-g$, $skymag-g$, $skymag-z$, $exptime-r$, $exptime-z$, $mjd-g$, $mjd-z$) are the ones that worsen the validation RMSE when being removed from the input layer}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Hyper-parameter tuning, training, and testing}{8}{subsubsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Important imaging maps identified by the backward feature elimination (\textit  {feature selection}) procedure for the five partitions used for the DR7 dataset. A darker color of a point within each partition represents a more important attribute identified by the feature selection procedure. Note that $nstar$ is identified as the most important attribute in all partitions, i.e., across the footprint.}}{8}{figure.6}}
\newlabel{fig:dr7ablation2}{{6}{8}{Important imaging maps identified by the backward feature elimination (\textit {feature selection}) procedure for the five partitions used for the DR7 dataset. A darker color of a point within each partition represents a more important attribute identified by the feature selection procedure. Note that $nstar$ is identified as the most important attribute in all partitions, i.e., across the footprint}{figure.6}{}}
\citation{ross2011ameliorating}
\citation{ross2011ameliorating}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The best hyper-parameters for each partition.}}{9}{table.3}}
\newlabel{tab:hparams}{{3}{9}{The best hyper-parameters for each partition}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Mitigation with Multivariate Linear Functions}{9}{subsection.3.3}}
\newlabel{eq:multvar}{{11}{9}{Mitigation with Multivariate Linear Functions}{equation.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Angular Clustering Statistics}{9}{subsection.3.4}}
\newlabel{subsec:ang_clustering}{{3.4}{9}{Angular Clustering Statistics}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}One point statistics}{9}{subsubsection.3.4.1}}
\newlabel{eq:nnbar_stat}{{12}{9}{One point statistics}{equation.12}{}}
\newlabel{eq:error_jack}{{13}{9}{One point statistics}{equation.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Two-point clustering statistics}{9}{subsubsection.3.4.2}}
\newlabel{eq:delta}{{14}{9}{Two-point clustering statistics}{equation.14}{}}
\newlabel{eq:deltasysbar}{{15}{9}{Two-point clustering statistics}{equation.15}{}}
\citation{scranton2002analysis,ross2011ameliorating}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A flow-chart of the backward feature elimination and hyper-parameter tuning for each partition. This entire process is performed five times for each of the five partitions/permutations such that the entire footprint is covered by aggregating different testing folds. }}{10}{figure.7}}
\newlabel{fig:splitapp}{{7}{10}{A flow-chart of the backward feature elimination and hyper-parameter tuning for each partition. This entire process is performed five times for each of the five partitions/permutations such that the entire footprint is covered by aggregating different testing folds}{figure.7}{}}
\newlabel{eq:ic}{{16}{10}{Two-point clustering statistics}{equation.16}{}}
\newlabel{eq:ancor}{{17}{10}{Two-point clustering statistics}{equation.17}{}}
\citation{gorski2005healpix}
\citation{ashley2012MNRAS,2012ApJ...761...14H}
\citation{myers2007clustering,huterer2013calibration}
\citation{1991MNRAS.248....1C}
\citation{hand2017nbodykit}
\citation{ade2016planck}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{white2013mock}
\citation{Raichoor2017MNRAS.471.3955R}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Survey Mocks}{11}{subsection.3.5}}
\newlabel{subsec:surveymocks}{{3.5}{11}{Survey Mocks}{subsection.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Twenty equal-area contiguous regions used to estimate the Jackknife errorbars for the 2D clustering statistics.}}{11}{figure.8}}
\newlabel{fig:jackknifes}{{8}{11}{Twenty equal-area contiguous regions used to estimate the Jackknife errorbars for the 2D clustering statistics}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Null Mocks}{11}{subsubsection.3.5.1}}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{Raichoor2017MNRAS.471.3955R}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The projection of the mock footprint (blue) onto the North Galactic Cap of the DECaLS DR7 footprint (gray). With this projection, the imaging attributes from the real data are assigned to the mocks.}}{12}{figure.9}}
\newlabel{fig:mock_on_dr}{{9}{12}{The projection of the mock footprint (blue) onto the North Galactic Cap of the DECaLS DR7 footprint (gray). With this projection, the imaging attributes from the real data are assigned to the mocks}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Contaminated Mocks}{12}{subsubsection.3.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Histogram of the number of galaxies per pixel for DR7 (hatched grey), null (solid blue), and contaminated (dashed red) mocks. All distributions are corrected for the pixel completeness $f_{\rm  pix}$. The DR7 distribution is scaled down to account for 5\% tiling completeness, and 27\% to 0.55 < reliable redshift <1.5. The residual difference between the mocks and the DR7 shown here might be due to differences between the \textit  {clean photometry} criteria applied to eBOSS target selection and those we apply to DR7.}}{12}{figure.10}}
\newlabel{fig:ngal_hist}{{10}{12}{Histogram of the number of galaxies per pixel for DR7 (hatched grey), null (solid blue), and contaminated (dashed red) mocks. All distributions are corrected for the pixel completeness $f_{\rm pix}$. The DR7 distribution is scaled down to account for 5\% tiling completeness, and 27\% to 0.55 < reliable redshift <1.5. The residual difference between the mocks and the DR7 shown here might be due to differences between the \textit {clean photometry} criteria applied to eBOSS target selection and those we apply to DR7}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{12}{section.4}}
\newlabel{sec:results}{{4}{12}{Results}{section.4}{}}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{ross2011ameliorating,2012ApJ...761...14H}
\citation{myers2007clustering,ross2007higher,huterer2013calibration}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Mitigating systematics from the DECaLS DR7 Data}{13}{subsection.4.1}}
\newlabel{sec:mitigateDR7}{{4.1}{13}{Mitigating systematics from the DECaLS DR7 Data}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces We use the $\chi ^{2}$ statistic to quantify the performance of each mitigation method based on $\overline  {n}/\overline  {n}_{tot}$ vs. systematics (see. Fig. \ref  {fig:nnbar}). In this table we show the cumulative value over all bins and all imaging attributes (i.e., $N_{\rm  bins}=$20 bins $\times $ 18 attributes) without accounting for the covariance both between the imaging maps and between different bins. Note that the best fit neural network model was applied to the unseen data (i.e., the test set) unlike in the linear and quadratic polynomial models. Nevertheless the neural network method returns the smallest $\chi ^{2}$, i.e., the highest efficiency.}}{13}{table.4}}
\newlabel{tab:chi2}{{4}{13}{We use the $\chi ^{2}$ statistic to quantify the performance of each mitigation method based on $\overline {n}/\overline {n}_{tot}$ vs. systematics (see. Fig. \ref {fig:nnbar}). In this table we show the cumulative value over all bins and all imaging attributes (i.e., $N_{\rm bins}=$20 bins $\times $ 18 attributes) without accounting for the covariance both between the imaging maps and between different bins. Note that the best fit neural network model was applied to the unseen data (i.e., the test set) unlike in the linear and quadratic polynomial models. Nevertheless the neural network method returns the smallest $\chi ^{2}$, i.e., the highest efficiency}{table.4}{}}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{ashley2012MNRAS,2012ApJ...761...14H}
\citation{Raichoor2017MNRAS.471.3955R}
\citation{ashley2012MNRAS,2012ApJ...761...14H}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textit  {Left}: Distribution of the selection masks (i.e., estimates of the contamination model) derived from different regression models. \textit  {Right}: Spatial scatter of the pixels we remove from our data due to the extreme values of the neural network selection mask.}}{14}{figure.11}}
\newlabel{fig:weights}{{11}{14}{\textit {Left}: Distribution of the selection masks (i.e., estimates of the contamination model) derived from different regression models. \textit {Right}: Spatial scatter of the pixels we remove from our data due to the extreme values of the neural network selection mask}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Testing the mitigation methods on the mock data}{14}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Feature selection of mock galaxies}{14}{subsubsection.4.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textit  {Top}: the normalized observed galaxy density and corrected density map using the neural network selection mask from left to right, respectively. \textit  {Bottom}: the selection masks from the Neural Network, quadratic, and linear polynomial models, respectively from left to right. All three selection masks are able to capture the behavior that the galaxy density systematically drops at the footprint boundaries i.e., high extinction regions.}}{15}{figure.12}}
\newlabel{fig:density_selection}{{12}{15}{\textit {Top}: the normalized observed galaxy density and corrected density map using the neural network selection mask from left to right, respectively. \textit {Bottom}: the selection masks from the Neural Network, quadratic, and linear polynomial models, respectively from left to right. All three selection masks are able to capture the behavior that the galaxy density systematically drops at the footprint boundaries i.e., high extinction regions}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Mean mock galaxy density}{15}{subsubsection.4.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Angular power spectrum of mock galaxies}{15}{subsubsection.4.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The number density of galaxies as a function of the potential systematics. The solid black curve shows the result before mitigation (\textit  {no correction}); the solid red curve is for the result after correcting with the neural network selection mask; the dot-dashed and dashed black curves represent mitigations with the linear and quadratic polynomial selection masks, respectively. The error bars are estimated using the Jackknife resampling of 20 non-contiguous subsamples of pixels within each imaging attribute bin (a total of 20 bins per attribute) and are shown only for one case. This plot again shows that the Galactic foregrounds such as the stellar density introduce a systematic trend in the galaxy density, which indicates a significant contamination by our own galaxy before mitigation. Such systematic trends are mostly removed with any of the three mitigation methods. }}{16}{figure.13}}
\newlabel{fig:nnbar}{{13}{16}{The number density of galaxies as a function of the potential systematics. The solid black curve shows the result before mitigation (\textit {no correction}); the solid red curve is for the result after correcting with the neural network selection mask; the dot-dashed and dashed black curves represent mitigations with the linear and quadratic polynomial selection masks, respectively. The error bars are estimated using the Jackknife resampling of 20 non-contiguous subsamples of pixels within each imaging attribute bin (a total of 20 bins per attribute) and are shown only for one case. This plot again shows that the Galactic foregrounds such as the stellar density introduce a systematic trend in the galaxy density, which indicates a significant contamination by our own galaxy before mitigation. Such systematic trends are mostly removed with any of the three mitigation methods}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The cross power spectrum $\mathaccentV {hat}05E{C}^{g,s_k}_{\ell }$ between the DR7 observed galaxy density and the imaging attributes $s_k$ normalized by the auto power spectrum of the imaging attribute $\mathaccentV {hat}05E{C}^{s_k,s_k}_{\ell }$. The plotted quantity $[\mathaccentV {hat}05E{C}^{g,s_k}_{\ell }]^2/\mathaccentV {hat}05E{C}^{s_k,s_k}_{\ell }$ approximately represents the level of contamination to the auto power spectrum of the galaxy density $\mathaccentV {hat}05E{C}^{g,g}_{\ell }$. The grey shaded region shows the Jackknife error estimate of $\mathaccentV {hat}05E{C}^{g,g}_{\ell }$. The black solid curve shows the result before mitigation (\textit  {no correction}), while the solid red curve shows the result after correcting for the systematics with the neural network selection mask. The dot-dashed and dashed black curves show the corrected results with the linear and quadratic polynomial model selection masks, respectively. }}{17}{figure.14}}
\newlabel{fig:clcross}{{14}{17}{The cross power spectrum $\hat {C}^{g,s_k}_{\ell }$ between the DR7 observed galaxy density and the imaging attributes $s_k$ normalized by the auto power spectrum of the imaging attribute $\hat {C}^{s_k,s_k}_{\ell }$. The plotted quantity $[\hat {C}^{g,s_k}_{\ell }]^2/\hat {C}^{s_k,s_k}_{\ell }$ approximately represents the level of contamination to the auto power spectrum of the galaxy density $\hat {C}^{g,g}_{\ell }$. The grey shaded region shows the Jackknife error estimate of $\hat {C}^{g,g}_{\ell }$. The black solid curve shows the result before mitigation (\textit {no correction}), while the solid red curve shows the result after correcting for the systematics with the neural network selection mask. The dot-dashed and dashed black curves show the corrected results with the linear and quadratic polynomial model selection masks, respectively}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The cross correlation function $\omega ^{g,s_k}(\theta )$ between the DR7 observed galaxy density and the imaging attributes $s_k$ normalized by the auto correlation function of the imaging attribute $\omega ^{s_k,s_k}(\theta )$. The plotted quantity $[\omega ^{g,s_k}(\theta )]^2/\omega ^{s_k,s_k}(\theta )$ approximately represents the level of contamination to the auto correlation function of the galaxy density $\omega ^{g,g}(\theta )$. The grey shaded region shows the Jackknife error estimate of $\omega ^{g,g}(\theta )$. All mitigation techniques are able to reduce the excess clustering singla which is due to the imaging systematics. }}{18}{figure.15}}
\newlabel{fig:xicross}{{15}{18}{The cross correlation function $\omega ^{g,s_k}(\theta )$ between the DR7 observed galaxy density and the imaging attributes $s_k$ normalized by the auto correlation function of the imaging attribute $\omega ^{s_k,s_k}(\theta )$. The plotted quantity $[\omega ^{g,s_k}(\theta )]^2/\omega ^{s_k,s_k}(\theta )$ approximately represents the level of contamination to the auto correlation function of the galaxy density $\omega ^{g,g}(\theta )$. The grey shaded region shows the Jackknife error estimate of $\omega ^{g,g}(\theta )$. All mitigation techniques are able to reduce the excess clustering singla which is due to the imaging systematics}{figure.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Cross power spectrum of mock galaxies and imaging attributes}{18}{subsubsection.4.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Two-point clustering statistics for the DECaLS DR7 dataset. \textit  {Left}: the measured angular power spectrum without shot-noise subtraction. \textit  {Right}: the HEALPIX-based angular correlation function. Solid black curves show the measured statistics without correcting for the systematic effects (\textit  {no correction}). The dashed and dot-dashed black curves show the statistics after correcting with linear and quadratic polynomial mitigation methods, respectively. The solid red curves show the results after correcting with our default neural network method. The dashed blue curves show the results mitigated with the neural network method but without the feature selection process. The dotted curves in both panels are a redshift-space linear theory prediction using $galaxy\tmspace  +\thickmuskip {.2777em}bias = 2$ and the surface density $n(z)$ of NGC eBOSS ELG \nobreakspace  {}\citep  [Tab. 4 of][]{Raichoor2017MNRAS.471.3955R} and assuming the fiducial cosmology of \citet  {ashley2012MNRAS,2012ApJ...761...14H}. In the left panel, while the data includes the survey window function effect, the theory model does not include the survey window effect. The estimated shot noise was added to the theory model in the left panel. The errors are estimated using the Jackknife resampling with 20 contiguous sub-regions (see Fig. \ref  {fig:jackknifes}).}}{19}{figure.16}}
\newlabel{fig:clxi}{{16}{19}{Two-point clustering statistics for the DECaLS DR7 dataset. \textit {Left}: the measured angular power spectrum without shot-noise subtraction. \textit {Right}: the HEALPIX-based angular correlation function. Solid black curves show the measured statistics without correcting for the systematic effects (\textit {no correction}). The dashed and dot-dashed black curves show the statistics after correcting with linear and quadratic polynomial mitigation methods, respectively. The solid red curves show the results after correcting with our default neural network method. The dashed blue curves show the results mitigated with the neural network method but without the feature selection process. The dotted curves in both panels are a redshift-space linear theory prediction using $galaxy\;bias = 2$ and the surface density $n(z)$ of NGC eBOSS ELG ~\citep [Tab. 4 of][]{Raichoor2017MNRAS.471.3955R} and assuming the fiducial cosmology of \citet {ashley2012MNRAS,2012ApJ...761...14H}. In the left panel, while the data includes the survey window function effect, the theory model does not include the survey window effect. The estimated shot noise was added to the theory model in the left panel. The errors are estimated using the Jackknife resampling with 20 contiguous sub-regions (see Fig. \ref {fig:jackknifes})}{figure.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}A case with underfitting}{19}{subsubsection.4.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6}Summary}{20}{subsubsection.4.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}conclusion}{20}{section.5}}
\newlabel{sec:conclusion}{{5}{20}{conclusion}{section.5}{}}
\bibstyle{mnras}
\bibdata{refs}
\bibcite{ade2016planck}{{1}{2016}{{Ade et~al.}}{{Ade et~al.,}}}
\bibcite{aghamousa2016desi}{{2}{2016}{{Aghamousa et~al.}}{{Aghamousa et~al.,}}}
\bibcite{ahn2012ninth}{{3}{2012}{{Ahn et~al.}}{{Ahn et~al.,}}}
\bibcite{akrami2018planck}{{4}{2018}{{Akrami et~al.}}{{Akrami et~al.,}}}
\bibcite{BOSSfinal}{{5}{2017}{{{Alam} et~al.}}{{{Alam} et~al.,}}}
\bibcite{ata2017clustering}{{6}{2017}{{Ata et~al.}}{{Ata et~al.,}}}
\bibcite{bekhti2016hi4pi}{{7}{2016}{{Bekhti et~al.}}{{Bekhti et~al.,}}}
\bibcite{berge2013ultra}{{8}{2013}{{Berg{\'e} et~al.}}{{Berg{\'e}, Gamper, R{\'e}fr{\'e}gier \& Amara}}}
\bibcite{breiman2001random}{{9}{2001}{{Breiman}}{{Breiman}}}
\bibcite{brown2018gaia}{{10}{2018}{{Brown et~al.}}{{Brown, Vallenari, Prusti, de Bruijne, Babusiaux, Bailer-Jones, Collaboration et~al.}}}
\bibcite{1991MNRAS.248....1C}{{11}{1991}{{{Coles} \& {Jones}}}{{{Coles} \& {Jones}}}}
\bibcite{colless20012df}{{12}{2001}{{Colless et~al.}}{{Colless et~al.,}}}
\bibcite{cybenko1989approximation}{{13}{1989}{{Cybenko}}{{Cybenko}}}
\bibcite{dahl2013improving}{{14}{2013}{{Dahl et~al.}}{{Dahl, Sainath \& Hinton}}}
\bibcite{dalal2008imprints}{{15}{2008}{{Dalal et~al.}}{{Dalal, Dore, Huterer \& Shirokov}}}
\bibcite{dark2005dark}{{16}{2005}{{Dark Energy Survey Collaboration:~Fermilab \& Flaugher}}{{Dark Energy Survey Collaboration:~Fermilab \& Flaugher}}}
\bibcite{delubac2016sdss}{{17}{2016}{{Delubac et~al.}}{{Delubac et~al.,}}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Important imaging maps identified by the feature selection procedure for all five partitions of the 100 null (left) and contaminated mocks (right). The maps used in the input contamination model were marked by `*' and horizontal black solid curves. }}{21}{figure.17}}
\newlabel{fig:mockablation}{{17}{21}{Important imaging maps identified by the feature selection procedure for all five partitions of the 100 null (left) and contaminated mocks (right). The maps used in the input contamination model were marked by `*' and horizontal black solid curves}{figure.17}{}}
\bibcite{devijver1982pattern}{{18}{1982}{{Devijver \& Kittler}}{{Devijver \& Kittler}}}
\bibcite{dey2018overview}{{19}{2018}{{Dey et~al.}}{{Dey et~al.,}}}
\bibcite{drinkwater2010wigglez}{{20}{2010}{{Drinkwater et~al.}}{{Drinkwater et~al.,}}}
\bibcite{efstathiou1988analysis}{{21}{1988}{{Efstathiou et~al.}}{{Efstathiou, Ellis \& Peterson}}}
\bibcite{eisenstein2005dark}{{22}{2005}{{Eisenstein}}{{Eisenstein}}}
\bibcite{eisenstein1998cosmic}{{23}{1998}{{Eisenstein et~al.}}{{Eisenstein, Hu \& Tegmark}}}
\bibcite{elsner2015unbiased}{{24}{2015}{{Elsner et~al.}}{{Elsner, Leistedt \& Peiris}}}
\bibcite{Elvin18}{{25}{2018}{{{Elvin-Poole} et~al.}}{{{Elvin-Poole} et~al.,}}}
\bibcite{fisher1993power}{{26}{1993}{{Fisher et~al.}}{{Fisher, Davis, Strauss, Yahil \& Huchra}}}
\bibcite{funahashi1989approximate}{{27}{1989}{{Funahashi}}{{Funahashi}}}
\bibcite{geurts2006extremely}{{28}{2006}{{Geurts et~al.}}{{Geurts, Ernst \& Wehenkel}}}
\bibcite{glorot2011deep}{{29}{2011}{{Glorot et~al.}}{{Glorot, Bordes \& Bengio}}}
\bibcite{gorski2005healpix}{{30}{2005}{{Gorski et~al.}}{{Gorski, Hivon, Banday, Wandelt, Hansen, Reinecke \& Bartelmann}}}
\bibcite{guyon2003introduction}{{31}{2003}{{Guyon \& Elisseeff}}{{Guyon \& Elisseeff}}}
\bibcite{hamilton1998linear}{{32}{1998}{{Hamilton}}{{Hamilton}}}
\bibcite{hand2017nbodykit}{{33}{2017}{{Hand et~al.}}{{Hand, Feng, Beutler, Li, Modi, Seljak \& Slepian}}}
\bibcite{he2015delving}{{34}{2015}{{He et~al.}}{{He, Zhang, Ren \& Sun}}}
\bibcite{ho2008correlation}{{35}{2008}{{Ho et~al.}}{{Ho, Hirata, Padmanabhan, Seljak \& Bahcall}}}
\bibcite{2012ApJ...761...14H}{{36}{2012}{{{Ho} et~al.}}{{{Ho} et~al.,}}}
\bibcite{hoerl1970ridge}{{37}{1970}{{Hoerl \& Kennard}}{{Hoerl \& Kennard}}}
\bibcite{hornik1989multilayer}{{38}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe \& White}}}
\bibcite{huang2003learning}{{39}{2003}{{Huang}}{{Huang}}}
\bibcite{huterer2013calibration}{{40}{2013}{{Huterer et~al.}}{{Huterer, Cunha \& Fang}}}
\bibcite{ivezic2008lsst}{{41}{2008}{{Ivezic et~al.}}{{Ivezic et~al.,}}}
\bibcite{john1994irrelevant}{{42}{1994}{{John et~al.}}{{John, Kohavi \& Pfleger}}}
\bibcite{jones2018measuring}{{43}{2018}{{Jones et~al.}}{{Jones et~al.,}}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The number density of mock galaxies as a function of the potential systematics averaged over 100 mock datasets. The grey shaded region illustrates 1$-\sigma $ dispersion in the null mocks. The dotted curve shows the mean density of the null mocks. The black solid curve shows the mean density dependence on the imaging attributes for the contaminated mocks. The solid red curve shows the mean density after correcting for the systematics with the neural network selection mask. The dashed black curve shows the corrected results with the quadratic polynomial model selection mask. The result of the linear polynomial model is almost unity, and therefore is omitted for clarity.}}{22}{figure.18}}
\newlabel{fig:nnbarmock}{{18}{22}{The number density of mock galaxies as a function of the potential systematics averaged over 100 mock datasets. The grey shaded region illustrates 1$-\sigma $ dispersion in the null mocks. The dotted curve shows the mean density of the null mocks. The black solid curve shows the mean density dependence on the imaging attributes for the contaminated mocks. The solid red curve shows the mean density after correcting for the systematics with the neural network selection mask. The dashed black curve shows the corrected results with the quadratic polynomial model selection mask. The result of the linear polynomial model is almost unity, and therefore is omitted for clarity}{figure.18}{}}
\bibcite{kaiser1987clustering}{{44}{1987}{{Kaiser}}{{Kaiser}}}
\bibcite{kalus2016unbiased}{{45}{2016}{{Kalus et~al.}}{{Kalus, Percival, Bacon \& Samushia}}}
\bibcite{kalus2018map}{{46}{2018}{{Kalus et~al.}}{{Kalus, Percival, Bacon, Mueller, Samushia, Verde, Ross \& Bernal}}}
\bibcite{kingma2014adam}{{47}{2014}{{Kingma \& Ba}}{{Kingma \& Ba}}}
\bibcite{kohavi1997wrappers}{{48}{1997}{{Kohavi \& John}}{{Kohavi \& John}}}
\bibcite{koller1996toward}{{49}{1996}{{Koller \& Sahami}}{{Koller \& Sahami}}}
\bibcite{krizhevsky2012imagenet}{{50}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever \& Hinton}}}
\bibcite{LSSTObservingStrategyWhitePaper}{{51}{2017}{{{LSST Science Collaborations} et~al.}}{{{LSST Science Collaborations} et~al.,}}}
\bibcite{lang2016tractor}{{52}{2016}{{Lang et~al.}}{{Lang, Hogg \& Mykytyn}}}
\bibcite{laurent2017clustering}{{53}{2017}{{Laurent et~al.}}{{Laurent et~al.,}}}
\bibcite{leistedt2014exploiting}{{54}{2014}{{Leistedt \& Peiris}}{{Leistedt \& Peiris}}}
\bibcite{leistedt2013estimating}{{55}{2013}{{Leistedt et~al.}}{{Leistedt, Peiris, Mortlock, Benoit-L{\'e}vy \& Pontzen}}}
\bibcite{LeistedtMap}{{56}{2016}{{{Leistedt} et~al.}}{{{Leistedt} et~al.,}}}
\bibcite{lin2017does}{{57}{2017}{{Lin et~al.}}{{Lin, Tegmark \& Rolnick}}}
\bibcite{mather1994measurement}{{58}{1994}{{Mather et~al.}}{{Mather et~al.,}}}
\bibcite{montufar2014number}{{59}{2014}{{Montufar et~al.}}{{Montufar, Pascanu, Cho \& Bengio}}}
\bibcite{mukhanov1992theory}{{60}{1992}{{Mukhanov et~al.}}{{Mukhanov, Feldman \& Brandenberger}}}
\bibcite{myers2007clustering}{{61}{2007}{{Myers et~al.}}{{Myers, Brunner, Richards, Nichol, Schneider \& Bahcall}}}
\bibcite{nair2010rectified}{{62}{2010}{{Nair \& Hinton}}{{Nair \& Hinton}}}
\bibcite{peebles1973statistical}{{63}{1973}{{Peebles}}{{Peebles}}}
\bibcite{perlmutter1999measurements}{{64}{1999}{{Perlmutter et~al.}}{{Perlmutter et~al.,}}}
\bibcite{prakash2016sdss}{{65}{2016}{{Prakash et~al.}}{{Prakash et~al.,}}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces \textit  {Top row}: The angular power spectrum of the contaminated (null) mocks in the right (left) panels. \textit  {Middle row}: The power spectrum subtracted by the mean of the null mocks to better visualize the remaining bias after each mitigation. The dark grey shaded region shows the 1$-\sigma $ confidence region of the mean of 100 mocks, while the light grey area shows the 1$-\sigma $ confidence region for one mock, calculated from the dispersion of 100 mocks. To account for the increased shot noise during contamination, we remove the same constant power from all contaminated/mitigated power spectra until their small scale power matches that of the null mock power spectrum. We quantify the significance of the remaining bias by calculating $\chi ^2$, the sum of the squared offset weighted with the diagonal variance of the mean $C_\ell $ of null mocks over six bins (with $\ell > 10$). The middle panel on the left illustrates the neural network without feature selection (`plain') tends to remove the cosmological clustering signal. }}{23}{figure.19}}
\newlabel{fig:deltaclmock}{{19}{23}{\textit {Top row}: The angular power spectrum of the contaminated (null) mocks in the right (left) panels. \textit {Middle row}: The power spectrum subtracted by the mean of the null mocks to better visualize the remaining bias after each mitigation. The dark grey shaded region shows the 1$-\sigma $ confidence region of the mean of 100 mocks, while the light grey area shows the 1$-\sigma $ confidence region for one mock, calculated from the dispersion of 100 mocks. To account for the increased shot noise during contamination, we remove the same constant power from all contaminated/mitigated power spectra until their small scale power matches that of the null mock power spectrum. We quantify the significance of the remaining bias by calculating $\chi ^2$, the sum of the squared offset weighted with the diagonal variance of the mean $C_\ell $ of null mocks over six bins (with $\ell > 10$). The middle panel on the left illustrates the neural network without feature selection (`plain') tends to remove the cosmological clustering signal}{figure.19}{}}
\bibcite{pullen2013systematic}{{66}{2013}{{Pullen \& Hirata}}{{Pullen \& Hirata}}}
\bibcite{Raichoor2017MNRAS.471.3955R}{{67}{2017}{{{Raichoor} et~al.}}{{{Raichoor} et~al.,}}}
\bibcite{ramaswamy2001multiclass}{{68}{2001}{{Ramaswamy et~al.}}{{Ramaswamy et~al.,}}}
\bibcite{riess1998observational}{{69}{1998}{{Riess et~al.}}{{Riess et~al.,}}}
\bibcite{rolnick2017power}{{70}{2017}{{Rolnick \& Tegmark}}{{Rolnick \& Tegmark}}}
\bibcite{ross2007higher}{{71}{2007}{{Ross et~al.}}{{Ross, Brunner \& Myers}}}
\bibcite{ross2011ameliorating}{{72}{2011}{{Ross et~al.}}{{Ross et~al.,}}}
\bibcite{ashley2012MNRAS}{{73}{2012}{{{Ross} et~al.}}{{{Ross} et~al.,}}}
\bibcite{rossfNL}{{74}{2013}{{Ross et~al.}}{{Ross et~al.,}}}
\bibcite{Ross17}{{75}{2017}{{{Ross} et~al.}}{{{Ross} et~al.,}}}
\bibcite{ruder2016overview}{{76}{2016}{{Ruder}}{{Ruder}}}
\bibcite{rybicki1992interpolation}{{77}{1992}{{Rybicki \& Press}}{{Rybicki \& Press}}}
\bibcite{sanchez2008best}{{78}{2008}{{S{\'a}nchez et~al.}}{{S{\'a}nchez, Baugh \& Angulo}}}
\bibcite{schlafly2011measuring}{{79}{2011}{{Schlafly \& Finkbeiner}}{{Schlafly \& Finkbeiner}}}
\bibcite{schlegel1998maps}{{80}{1998}{{Schlegel et~al.}}{{Schlegel, Finkbeiner \& Davis}}}
\bibcite{scranton2002analysis}{{81}{2002}{{Scranton et~al.}}{{Scranton et~al.,}}}
\bibcite{seo2003probing}{{82}{2003}{{Seo \& Eisenstein}}{{Seo \& Eisenstein}}}
\bibcite{slosar2004exact}{{83}{2004}{{Slosar et~al.}}{{Slosar, Seljak \& Makarov}}}
\bibcite{smoot1992structure}{{84}{1992}{{Smoot et~al.}}{{Smoot et~al.,}}}
\bibcite{Balrog}{{85}{2016}{{{Suchyta} et~al.}}{{{Suchyta} et~al.,}}}
\bibcite{tamura1997capabilities}{{86}{1997}{{Tamura \& Tateishi}}{{Tamura \& Tateishi}}}
\bibcite{tegmark1997measure}{{87}{1997}{{Tegmark}}{{Tegmark}}}
\bibcite{tegmark1998measuring}{{88}{1998}{{Tegmark et~al.}}{{Tegmark, Hamilton, Strauss, Vogeley \& Szalay}}}
\bibcite{thomas2011excess}{{89}{2011a}{{Thomas et~al.}}{{Thomas, Abdalla \& Lahav}}}
\bibcite{thomas2011angular}{{90}{2011b}{{Thomas et~al.}}{{Thomas, Abdalla \& Lahav}}}
\bibcite{white2013mock}{{91}{2013}{{White et~al.}}{{White, Tinker \& McBride}}}
\bibcite{wright2010wide}{{92}{2010}{{Wright et~al.}}{{Wright et~al.,}}}
\bibcite{york2000sloan}{{93}{2000}{{York et~al.}}{{York et~al.,}}}
\bibcite{zou2017project}{{94}{2017}{{Zou et~al.}}{{Zou et~al.,}}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Dependence of $\chi ^{2}$ on the lowest bin $\ell _{\rm  min}$. To better quantify the residual bias introduced by each method, we evaluate the dependence of the bias on the lowest bin $\ell _{{\rm  min}}$ that is included in the $\chi ^{2}$ computation. The default neural network method performs significantly better than the conventional methods for the null mocks, mainly because the feature selection procedure successfully prevents the method from regressing out the cosmological clustering signal. For the contaminated mocks, all methods tend to perform similarly, as expected, since all mitigation methods can reproduce the input contamination model. }}{24}{figure.20}}
\newlabel{fig:chi2clmock}{{20}{24}{Dependence of $\chi ^{2}$ on the lowest bin $\ell _{\rm min}$. To better quantify the residual bias introduced by each method, we evaluate the dependence of the bias on the lowest bin $\ell _{{\rm min}}$ that is included in the $\chi ^{2}$ computation. The default neural network method performs significantly better than the conventional methods for the null mocks, mainly because the feature selection procedure successfully prevents the method from regressing out the cosmological clustering signal. For the contaminated mocks, all methods tend to perform similarly, as expected, since all mitigation methods can reproduce the input contamination model}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Cross power spectrum of the contaminated mock catalogs and the imaging attributes for different mitigation techniques. Our defaul neural network method with feature selection is shown in solid red while the performance without feature selection (`Neural Network (Plain)') is shown in dashed blue. The dark grey shaded region shows the 1$\sigma $ confidence region of the mean of the 100 mocks, and the light grey region shows the typical 1-$\sigma $ confidence region of one mock. The mitigation with the ground truth contamination model is shown with a purple dot-dashed curve as `cont. with linear (truth)'. }}{25}{figure.21}}
\newlabel{fig:clcrossmock}{{21}{25}{Cross power spectrum of the contaminated mock catalogs and the imaging attributes for different mitigation techniques. Our defaul neural network method with feature selection is shown in solid red while the performance without feature selection (`Neural Network (Plain)') is shown in dashed blue. The dark grey shaded region shows the 1$\sigma $ confidence region of the mean of the 100 mocks, and the light grey region shows the typical 1-$\sigma $ confidence region of one mock. The mitigation with the ground truth contamination model is shown with a purple dot-dashed curve as `cont. with linear (truth)'}{figure.21}{}}
\newlabel{lastpage}{{5}{25}{Acknowledgements}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Same as Fig. \ref  {fig:deltaclmock} showing the auto power spectrum results mitigated with the fewer imaging maps `few' (to demonstrate under-correction), neural network without the feature selection `plain', and the ground truth contamination model `truth'. The mitigation with the ground truth model achieves the lowest residual bias as expected. For the null mocks, using fewer imaging maps prevents over-fitting simply by providing less freedom in the regression model while it leads to underfitting for the contaminated mocks. }}{26}{figure.22}}
\newlabel{fig:mockdclextra}{{22}{26}{Same as Fig. \ref {fig:deltaclmock} showing the auto power spectrum results mitigated with the fewer imaging maps `few' (to demonstrate under-correction), neural network without the feature selection `plain', and the ground truth contamination model `truth'. The mitigation with the ground truth model achieves the lowest residual bias as expected. For the null mocks, using fewer imaging maps prevents over-fitting simply by providing less freedom in the regression model while it leads to underfitting for the contaminated mocks}{figure.22}{}}
